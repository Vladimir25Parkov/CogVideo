{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMC3DN4DLtM95vlu9O9Zqu6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "677ea76130534ab0b660cdea07aee519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_501cf8d153d441f39b087c96baaf7233",
              "IPY_MODEL_94ae9ffe0b6d4eba9cb1fd59f30f5d2d",
              "IPY_MODEL_5ce67d47eda142e8a1179ed999e87d9d"
            ],
            "layout": "IPY_MODEL_580514737c2042ffa2e570b4e56f280b"
          }
        },
        "501cf8d153d441f39b087c96baaf7233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41bb3ecd2010425e87c2b6dea847f1ef",
            "placeholder": "​",
            "style": "IPY_MODEL_c292b9073755491caaaaea77b4389a93",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "94ae9ffe0b6d4eba9cb1fd59f30f5d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94140258e07f4dde80118c41923877d1",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9112995a165c4ba6ac6e44d32500609e",
            "value": 28
          }
        },
        "5ce67d47eda142e8a1179ed999e87d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9704c38d28544e3b7dd73c3d9ac9e87",
            "placeholder": "​",
            "style": "IPY_MODEL_fc446dcb4abb4fcdaa9fc58a496ef39d",
            "value": " 28.0/28.0 [00:00&lt;00:00, 899B/s]"
          }
        },
        "580514737c2042ffa2e570b4e56f280b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41bb3ecd2010425e87c2b6dea847f1ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c292b9073755491caaaaea77b4389a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94140258e07f4dde80118c41923877d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9112995a165c4ba6ac6e44d32500609e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9704c38d28544e3b7dd73c3d9ac9e87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc446dcb4abb4fcdaa9fc58a496ef39d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a538579ee93646ceb952ec4cdd179965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa86c066288648fa8bec01bde96bdbdf",
              "IPY_MODEL_9dc044264eed411fa0087a371c937d3d",
              "IPY_MODEL_802ec20963694e5ab725103fff058aef"
            ],
            "layout": "IPY_MODEL_d045f2e082e74f1497435a701431e31f"
          }
        },
        "fa86c066288648fa8bec01bde96bdbdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db115b49296e45cb9931003e10fc0731",
            "placeholder": "​",
            "style": "IPY_MODEL_9c96ee5f48dc47eb9d0b7bca03b41ce5",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "9dc044264eed411fa0087a371c937d3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_284672a4205342248d30f3f389cc7fea",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_492b6c6ecda34dae96b7da116a414fae",
            "value": 570
          }
        },
        "802ec20963694e5ab725103fff058aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_109781c3dad14b2f852e8701f4c52fd9",
            "placeholder": "​",
            "style": "IPY_MODEL_962c502aa54d40609d6c626feec4be3d",
            "value": " 570/570 [00:00&lt;00:00, 17.1kB/s]"
          }
        },
        "d045f2e082e74f1497435a701431e31f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db115b49296e45cb9931003e10fc0731": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c96ee5f48dc47eb9d0b7bca03b41ce5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "284672a4205342248d30f3f389cc7fea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "492b6c6ecda34dae96b7da116a414fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "109781c3dad14b2f852e8701f4c52fd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "962c502aa54d40609d6c626feec4be3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0e3530002564b04a563420c7be5a084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a4a7d8225f2423ea54834cdb94f1b97",
              "IPY_MODEL_7de0ca5606bb403c841f48b6797253b0",
              "IPY_MODEL_e5aea9aabb764f24a77b2594f343a5d6"
            ],
            "layout": "IPY_MODEL_11682ce034bb4bc19af4287d206224f7"
          }
        },
        "6a4a7d8225f2423ea54834cdb94f1b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e30f514ca63749b9a0e7fd7248a6e50f",
            "placeholder": "​",
            "style": "IPY_MODEL_fa2b0243bf7c4a1eba4c21b8203c6a96",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "7de0ca5606bb403c841f48b6797253b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_478e9d5cc87c47f88debae72dde805c0",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da6fe2d49dfa4b2fb10d7633e93e3883",
            "value": 231508
          }
        },
        "e5aea9aabb764f24a77b2594f343a5d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efcc72faf7494112bc480f13e1d727c9",
            "placeholder": "​",
            "style": "IPY_MODEL_97ad2213b1784a1ca49be56063bc517a",
            "value": " 232k/232k [00:00&lt;00:00, 7.92MB/s]"
          }
        },
        "11682ce034bb4bc19af4287d206224f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e30f514ca63749b9a0e7fd7248a6e50f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa2b0243bf7c4a1eba4c21b8203c6a96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "478e9d5cc87c47f88debae72dde805c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da6fe2d49dfa4b2fb10d7633e93e3883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "efcc72faf7494112bc480f13e1d727c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97ad2213b1784a1ca49be56063bc517a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca93b74811054768b0ea703f4f698376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8e52cba7ca5410a85f03d3ff8a377c2",
              "IPY_MODEL_13a881b69e5d4273936ba8199b822791",
              "IPY_MODEL_57bdac11326448e0ae57563a551286d3"
            ],
            "layout": "IPY_MODEL_0c3ea10698c54612b83d568efebc22c7"
          }
        },
        "c8e52cba7ca5410a85f03d3ff8a377c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fdc63359a894308b370d43fe8d0cc89",
            "placeholder": "​",
            "style": "IPY_MODEL_c640b5de21de406dbd067a5334a6858f",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "13a881b69e5d4273936ba8199b822791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8b848a7cbd2440ea17e4ea28aebe273",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12e4d679908246e6a7a4f9c906e01dd0",
            "value": 440473133
          }
        },
        "57bdac11326448e0ae57563a551286d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5b41b2a762a4224900e139899b8b7b4",
            "placeholder": "​",
            "style": "IPY_MODEL_0b9cc5cf54d148f5a41461bccbfd11d4",
            "value": " 440M/440M [00:02&lt;00:00, 216MB/s]"
          }
        },
        "0c3ea10698c54612b83d568efebc22c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fdc63359a894308b370d43fe8d0cc89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c640b5de21de406dbd067a5334a6858f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8b848a7cbd2440ea17e4ea28aebe273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12e4d679908246e6a7a4f9c906e01dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5b41b2a762a4224900e139899b8b7b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b9cc5cf54d148f5a41461bccbfd11d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vladimir25Parkov/CogVideo/blob/main/Andrew_Huberman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bdBXVVYUo6nh",
        "outputId": "16b174cf-b39b-4d17-dcc0-c5af7160b278"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nemo_toolkit[nlp]\n",
            "  Downloading nemo_toolkit-1.18.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub (from nemo_toolkit[nlp])\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (0.56.4)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (1.22.4)\n",
            "Collecting onnx>=1.7.0 (from nemo_toolkit[nlp])\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (2.8.2)\n",
            "Collecting ruamel.yaml (from nemo_toolkit[nlp])\n",
            "  Downloading ruamel.yaml-0.17.26-py3-none-any.whl (109 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 kB\u001b[0m \u001b[31m783.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (1.2.2)\n",
            "Collecting setuptools==65.5.1 (from nemo_toolkit[nlp])\n",
            "  Downloading setuptools-65.5.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (2.12.2)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (1.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (4.65.0)\n",
            "Collecting wget (from nemo_toolkit[nlp])\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (1.14.1)\n",
            "Collecting boto3 (from nemo_toolkit[nlp])\n",
            "  Downloading boto3-1.26.137-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from nemo_toolkit[nlp])\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu (from nemo_toolkit[nlp])\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext (from nemo_toolkit[nlp])\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flask-restful (from nemo_toolkit[nlp])\n",
            "  Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n",
            "Collecting ftfy (from nemo_toolkit[nlp])\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (4.6.6)\n",
            "Collecting gradio (from nemo_toolkit[nlp])\n",
            "  Downloading gradio-3.32.0-py3-none-any.whl (19.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (3.8.0)\n",
            "Collecting ijson (from nemo_toolkit[nlp])\n",
            "  Downloading ijson-3.2.0.post0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (113 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.3/113.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (6.0.4)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (0.42.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (3.7.1)\n",
            "Collecting megatron-core==0.1.0 (from nemo_toolkit[nlp])\n",
            "  Downloading megatron_core-0.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (3.8.1)\n",
            "Collecting opencc (from nemo_toolkit[nlp])\n",
            "  Downloading OpenCC-1.1.6-cp310-cp310-manylinux1_x86_64.whl (778 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.3/778.3 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pangu (from nemo_toolkit[nlp])\n",
            "  Downloading pangu-4.0.6.1-py3-none-any.whl (6.4 kB)\n",
            "Collecting rapidfuzz (from nemo_toolkit[nlp])\n",
            "  Downloading rapidfuzz-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score (from nemo_toolkit[nlp])\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu[ja] (from nemo_toolkit[nlp])\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers (from nemo_toolkit[nlp])\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hydra-core<1.3,>=1.2.0 (from nemo_toolkit[nlp])\n",
            "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.1/151.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.3,>=2.2 (from nemo_toolkit[nlp])\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning<=1.9.4,>=1.9.0 (from nemo_toolkit[nlp])\n",
            "  Downloading pytorch_lightning-1.9.4-py3-none-any.whl (827 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.8/827.8 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml<6 (from nemo_toolkit[nlp])\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torchmetrics>=0.11.0 (from nemo_toolkit[nlp])\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.0.1 (from nemo_toolkit[nlp])\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb (from nemo_toolkit[nlp])\n",
            "  Downloading wandb-0.15.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webdataset<=0.1.62,>=0.1.48 (from nemo_toolkit[nlp])\n",
            "  Downloading webdataset-0.1.62-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (1.5.3)\n",
            "Collecting sacremoses>=0.0.43 (from nemo_toolkit[nlp])\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece<1.0.0 (from nemo_toolkit[nlp])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting youtokentome>=1.0.5 (from nemo_toolkit[nlp])\n",
            "  Downloading youtokentome-1.0.6.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core<1.3,>=1.2.0->nemo_toolkit[nlp])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.3,>=1.2.0->nemo_toolkit[nlp]) (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->nemo_toolkit[nlp]) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->nemo_toolkit[nlp]) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->nemo_toolkit[nlp]) (2022.10.31)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->nemo_toolkit[nlp]) (1.16.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[nlp]) (2023.4.0)\n",
            "Collecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[nlp])\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[nlp]) (3.12.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[nlp]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[nlp]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[nlp]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[nlp]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->nemo_toolkit[nlp]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->nemo_toolkit[nlp]) (16.0.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.1->nemo_toolkit[nlp]) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.0.1->nemo_toolkit[nlp])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting braceexpand (from webdataset<=0.1.62,>=0.1.48->nemo_toolkit[nlp])\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.137 (from boto3->nemo_toolkit[nlp])\n",
            "  Downloading botocore-1.29.137-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->nemo_toolkit[nlp])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0 (from boto3->nemo_toolkit[nlp])\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11>=2.2 (from fasttext->nemo_toolkit[nlp])\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Collecting aniso8601>=0.82 (from flask-restful->nemo_toolkit[nlp])\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo_toolkit[nlp]) (2.2.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo_toolkit[nlp]) (2022.7.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->nemo_toolkit[nlp]) (0.2.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->nemo_toolkit[nlp]) (4.11.2)\n",
            "Collecting aiofiles (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting aiohttp (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->nemo_toolkit[nlp]) (4.2.2)\n",
            "Collecting fastapi (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading fastapi-0.95.2-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.4 (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading gradio_client-0.2.5-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio->nemo_toolkit[nlp]) (2.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio->nemo_toolkit[nlp]) (2.1.2)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading orjson-3.8.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio->nemo_toolkit[nlp]) (1.10.7)\n",
            "Collecting pydub (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio->nemo_toolkit[nlp]) (2.14.0)\n",
            "Collecting python-multipart (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio->nemo_toolkit[nlp])\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->nemo_toolkit[nlp]) (0.39.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score->nemo_toolkit[nlp]) (1.4.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->nemo_toolkit[nlp])\n",
            "  Downloading ruamel.yaml.clib-0.2.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (485 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.6/485.6 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu[ja]->nemo_toolkit[nlp])\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu[ja]->nemo_toolkit[nlp]) (0.8.10)\n",
            "Collecting colorama (from sacrebleu[ja]->nemo_toolkit[nlp])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu[ja]->nemo_toolkit[nlp]) (4.9.2)\n",
            "Collecting mecab-python3==1.0.5 (from sacrebleu[ja]->nemo_toolkit[nlp])\n",
            "  Downloading mecab_python3-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.1/581.1 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipadic<2.0,>=1.0 (from sacrebleu[ja]->nemo_toolkit[nlp])\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nemo_toolkit[nlp]) (1.10.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nemo_toolkit[nlp]) (3.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->nemo_toolkit[nlp]) (0.15.2+cu118)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (1.54.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (2.3.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (0.40.0)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->nemo_toolkit[nlp])\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[nlp]) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->nemo_toolkit[nlp])\n",
            "  Downloading sentry_sdk-1.23.1-py2.py3-none-any.whl (205 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->nemo_toolkit[nlp])\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->nemo_toolkit[nlp])\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->nemo_toolkit[nlp])\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[nlp]) (1.4.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->nemo_toolkit[nlp]) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->nemo_toolkit[nlp]) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->nemo_toolkit[nlp]) (0.12.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.137->boto3->nemo_toolkit[nlp]) (1.26.15)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-restful->nemo_toolkit[nlp]) (2.1.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->nemo_toolkit[nlp]) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->nemo_toolkit[nlp]) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->gradio->nemo_toolkit[nlp])\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->gradio->nemo_toolkit[nlp])\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->gradio->nemo_toolkit[nlp])\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->gradio->nemo_toolkit[nlp])\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m958.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->gradio->nemo_toolkit[nlp])\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[nlp])\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[nlp]) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[nlp]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[nlp]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->nemo_toolkit[nlp]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->nemo_toolkit[nlp]) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio->nemo_toolkit[nlp])\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[nlp]) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[nlp]) (3.4)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio->nemo_toolkit[nlp])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->nemo_toolkit[nlp]) (2.4.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio->nemo_toolkit[nlp])\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio->nemo_toolkit[nlp])\n",
            "  Downloading httpcore-0.17.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->nemo_toolkit[nlp]) (1.3.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[nlp]) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->nemo_toolkit[nlp]) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[nlp])\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio->nemo_toolkit[nlp]) (3.6.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->nemo_toolkit[nlp]) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio->nemo_toolkit[nlp])\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[nlp]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->nemo_toolkit[nlp]) (3.2.2)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, pyyaml, sacremoses, youtokentome, fasttext, rouge-score, sentence-transformers, wget, ipadic, ffmpy, pathtools\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=edc391ce7a04c76d0fd1464e41936b761d6b6132af27f505221cdb10ed3d7974\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for pyyaml (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45658 sha256=643ee6e44f717ba0aae6c8bebde6e0fdc9658efaa93d56421798e3cbabfef9df\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=c88757e2b70126edaff963807ae6daf8b96106f839669371e2833dc8272fbd67\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for youtokentome (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=1927601 sha256=0476c25af200fd94589e1c35a156f664cb65a6a84727a83aef94f867158aa3a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/85/f8/301d2ba45f43f30bed2fe413efa760bc726b8b660ed9c2900c\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4393162 sha256=d05cb2cee542c5b6c1a34b014e9a99588b12524309b71a70de2a804f45687049\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=46c5ec87ca9396e8af9328849a8b427438d2db05c8e8f5fa799ba214406d3329\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=32174eebc44c851fa5cc6cdfabb380674a0b3f76b1b5839e5712960d63dc2ecc\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=da8e82020fe281ce20405521fcb6bad03b40f8b3f32dddb3b95aa74a3d8bec6f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556703 sha256=1e477eb666a2dd842505adb042670cb89978831f693eed1a083c6dd474f22173\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=f27c6472243545a59b019308754d899366d2288e4c334d2bd8c125b6370a9c8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=76da17b4d0aa3e7ba5c7c150b06309afc2239b3e00edc52e3bc62426271145c9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built antlr4-python3-runtime pyyaml sacremoses youtokentome fasttext rouge-score sentence-transformers wget ipadic ffmpy pathtools\n",
            "Installing collected packages: wget, tokenizers, sentencepiece, pydub, pathtools, pangu, opencc, mecab-python3, ipadic, ijson, ffmpy, faiss-cpu, braceexpand, antlr4-python3-runtime, aniso8601, youtokentome, websockets, webdataset, uc-micro-py, smmap, setuptools, setproctitle, sentry-sdk, semantic-version, sacremoses, ruamel.yaml.clib, rapidfuzz, pyyaml, python-multipart, pybind11, portalocker, orjson, onnx, multidict, lightning-utilities, jmespath, h11, ftfy, frozenlist, einops, docker-pycreds, colorama, async-timeout, aiofiles, yarl, uvicorn, starlette, sacrebleu, ruamel.yaml, rouge-score, omegaconf, mdit-py-plugins, linkify-it-py, huggingface-hub, httpcore, gitdb, fasttext, botocore, aiosignal, transformers, s3transfer, hydra-core, httpx, GitPython, flask-restful, fastapi, aiohttp, wandb, gradio-client, boto3, gradio, torchmetrics, sentence-transformers, pytorch-lightning, nemo_toolkit, megatron-core\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 65.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.31 aiofiles-23.1.0 aiohttp-3.8.4 aiosignal-1.3.1 aniso8601-9.0.1 antlr4-python3-runtime-4.9.3 async-timeout-4.0.2 boto3-1.26.137 botocore-1.29.137 braceexpand-0.1.7 colorama-0.4.6 docker-pycreds-0.4.0 einops-0.6.1 faiss-cpu-1.7.4 fastapi-0.95.2 fasttext-0.9.2 ffmpy-0.3.0 flask-restful-0.3.10 frozenlist-1.3.3 ftfy-6.1.1 gitdb-4.0.10 gradio-3.32.0 gradio-client-0.2.5 h11-0.14.0 httpcore-0.17.1 httpx-0.24.1 huggingface-hub-0.14.1 hydra-core-1.2.0 ijson-3.2.0.post0 ipadic-1.0.0 jmespath-1.0.1 lightning-utilities-0.8.0 linkify-it-py-2.0.2 mdit-py-plugins-0.3.3 mecab-python3-1.0.5 megatron-core-0.1.0 multidict-6.0.4 nemo_toolkit-1.18.1 omegaconf-2.2.3 onnx-1.14.0 opencc-1.1.6 orjson-3.8.12 pangu-4.0.6.1 pathtools-0.1.2 portalocker-2.7.0 pybind11-2.10.4 pydub-0.25.1 python-multipart-0.0.6 pytorch-lightning-1.9.4 pyyaml-5.4.1 rapidfuzz-3.0.0 rouge-score-0.1.2 ruamel.yaml-0.17.26 ruamel.yaml.clib-0.2.7 s3transfer-0.6.1 sacrebleu-2.3.1 sacremoses-0.0.53 semantic-version-2.10.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 sentry-sdk-1.23.1 setproctitle-1.3.2 setuptools-65.5.1 smmap-5.0.0 starlette-0.27.0 tokenizers-0.13.3 torchmetrics-0.11.4 transformers-4.29.2 uc-micro-py-1.0.2 uvicorn-0.22.0 wandb-0.15.3 webdataset-0.1.62 websockets-11.0.3 wget-3.2 yarl-1.9.2 youtokentome-1.0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "pydevd_plugins",
                  "setuptools"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Install nemo_toolkit\n",
        "\n",
        "# !pip install nemo_toolkit[nlp]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nemo.collections.nlp.models import PunctuationCapitalizationModel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv0Xpow4p5vP",
        "outputId": "3efb8217-516e-42aa-9d2e-0d9491152c3e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2023-05-21 14:12:45 optimizers:54] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
            "[NeMo W 2023-05-21 14:12:51 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = PunctuationCapitalizationModel.from_pretrained(\"punctuation_en_bert\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "677ea76130534ab0b660cdea07aee519",
            "501cf8d153d441f39b087c96baaf7233",
            "94ae9ffe0b6d4eba9cb1fd59f30f5d2d",
            "5ce67d47eda142e8a1179ed999e87d9d",
            "580514737c2042ffa2e570b4e56f280b",
            "41bb3ecd2010425e87c2b6dea847f1ef",
            "c292b9073755491caaaaea77b4389a93",
            "94140258e07f4dde80118c41923877d1",
            "9112995a165c4ba6ac6e44d32500609e",
            "f9704c38d28544e3b7dd73c3d9ac9e87",
            "fc446dcb4abb4fcdaa9fc58a496ef39d",
            "a538579ee93646ceb952ec4cdd179965",
            "fa86c066288648fa8bec01bde96bdbdf",
            "9dc044264eed411fa0087a371c937d3d",
            "802ec20963694e5ab725103fff058aef",
            "d045f2e082e74f1497435a701431e31f",
            "db115b49296e45cb9931003e10fc0731",
            "9c96ee5f48dc47eb9d0b7bca03b41ce5",
            "284672a4205342248d30f3f389cc7fea",
            "492b6c6ecda34dae96b7da116a414fae",
            "109781c3dad14b2f852e8701f4c52fd9",
            "962c502aa54d40609d6c626feec4be3d",
            "f0e3530002564b04a563420c7be5a084",
            "6a4a7d8225f2423ea54834cdb94f1b97",
            "7de0ca5606bb403c841f48b6797253b0",
            "e5aea9aabb764f24a77b2594f343a5d6",
            "11682ce034bb4bc19af4287d206224f7",
            "e30f514ca63749b9a0e7fd7248a6e50f",
            "fa2b0243bf7c4a1eba4c21b8203c6a96",
            "478e9d5cc87c47f88debae72dde805c0",
            "da6fe2d49dfa4b2fb10d7633e93e3883",
            "efcc72faf7494112bc480f13e1d727c9",
            "97ad2213b1784a1ca49be56063bc517a",
            "ca93b74811054768b0ea703f4f698376",
            "c8e52cba7ca5410a85f03d3ff8a377c2",
            "13a881b69e5d4273936ba8199b822791",
            "57bdac11326448e0ae57563a551286d3",
            "0c3ea10698c54612b83d568efebc22c7",
            "5fdc63359a894308b370d43fe8d0cc89",
            "c640b5de21de406dbd067a5334a6858f",
            "b8b848a7cbd2440ea17e4ea28aebe273",
            "12e4d679908246e6a7a4f9c906e01dd0",
            "f5b41b2a762a4224900e139899b8b7b4",
            "0b9cc5cf54d148f5a41461bccbfd11d4"
          ]
        },
        "id": "r_KR7l6FqA9v",
        "outputId": "36da6018-fd62-41a2-e4ec-c429d3579dc9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-21 14:13:09 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/punctuation_en_bert/versions/1.0.0rc1/files/punctuation_en_bert.nemo to /root/.cache/torch/NeMo/NeMo_1.18.1/punctuation_en_bert/93b0369b5e0d147f61895feffcbcfb88/punctuation_en_bert.nemo\n",
            "[NeMo I 2023-05-21 14:13:22 common:913] Instantiating model from pre-trained checkpoint\n",
            "[NeMo I 2023-05-21 14:13:28 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmp3s6l3nwt/tokenizer.vocab_file, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "677ea76130534ab0b660cdea07aee519"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a538579ee93646ceb952ec4cdd179965"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0e3530002564b04a563420c7be5a084"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using eos_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "[NeMo W 2023-05-21 14:13:29 modelPT:245] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
            "[NeMo W 2023-05-21 14:13:29 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    use_audio: false\n",
            "    audio_file: null\n",
            "    sample_rate: 16000\n",
            "    use_bucketing: true\n",
            "    batch_size: 32\n",
            "    preload_audios: true\n",
            "    use_tarred_dataset: false\n",
            "    label_info_save_dir: null\n",
            "    text_file: text_train.txt\n",
            "    labels_file: labels_train.txt\n",
            "    tokens_in_batch: null\n",
            "    max_seq_length: 128\n",
            "    num_samples: -1\n",
            "    use_cache: true\n",
            "    cache_dir: null\n",
            "    get_label_frequences: false\n",
            "    verbose: true\n",
            "    n_jobs: 0\n",
            "    tar_metadata_file: null\n",
            "    tar_shuffle_n: 1\n",
            "    shard_strategy: scatter\n",
            "    shuffle: true\n",
            "    drop_last: false\n",
            "    pin_memory: true\n",
            "    num_workers: 8\n",
            "    persistent_workers: true\n",
            "    ds_item: punct_dataset_complete\n",
            "    \n",
            "[NeMo W 2023-05-21 14:13:29 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    use_audio: false\n",
            "    audio_file: null\n",
            "    sample_rate: 16000\n",
            "    use_bucketing: true\n",
            "    batch_size: 32\n",
            "    preload_audios: true\n",
            "    use_tarred_dataset: false\n",
            "    label_info_save_dir: null\n",
            "    text_file: text_dev.txt\n",
            "    labels_file: labels_dev.txt\n",
            "    tokens_in_batch: null\n",
            "    max_seq_length: 128\n",
            "    num_samples: -1\n",
            "    use_cache: true\n",
            "    cache_dir: null\n",
            "    get_label_frequences: false\n",
            "    verbose: true\n",
            "    n_jobs: 0\n",
            "    tar_metadata_file: null\n",
            "    tar_shuffle_n: 1\n",
            "    shard_strategy: scatter\n",
            "    shuffle: true\n",
            "    drop_last: false\n",
            "    pin_memory: true\n",
            "    num_workers: 8\n",
            "    persistent_workers: true\n",
            "    ds_item: punct_dataset_complete\n",
            "    \n",
            "[NeMo W 2023-05-21 14:13:29 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    use_audio: false\n",
            "    audio_file: null\n",
            "    sample_rate: 16000\n",
            "    use_bucketing: true\n",
            "    batch_size: 32\n",
            "    preload_audios: true\n",
            "    use_tarred_dataset: false\n",
            "    label_info_save_dir: null\n",
            "    text_file: text_dev.txt\n",
            "    labels_file: labels_dev.txt\n",
            "    tokens_in_batch: null\n",
            "    max_seq_length: 128\n",
            "    num_samples: -1\n",
            "    use_cache: true\n",
            "    cache_dir: null\n",
            "    get_label_frequences: false\n",
            "    verbose: true\n",
            "    n_jobs: 0\n",
            "    tar_metadata_file: null\n",
            "    tar_shuffle_n: 1\n",
            "    shard_strategy: scatter\n",
            "    shuffle: true\n",
            "    drop_last: false\n",
            "    pin_memory: true\n",
            "    num_workers: 8\n",
            "    persistent_workers: true\n",
            "    ds_item: punct_dataset_complete\n",
            "    \n",
            "[NeMo W 2023-05-21 14:13:29 nlp_overrides:248] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
            "    Megatron-based models require Apex to function correctly.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca93b74811054768b0ea703f4f698376"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertEncoder: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[NeMo W 2023-05-21 14:13:37 punctuation_capitalization_model:705] The artifact `class_labels.punct_labels_file` was not found in checkpoint. Will rely on `punct_label_ids` parameter\n",
            "[NeMo W 2023-05-21 14:13:37 punctuation_capitalization_model:727] The artifact `class_labels.capit_labels_file` was not found in checkpoint. Will rely on `capit_label_ids` parameter\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-21 14:13:37 save_restore_connector:249] Model PunctuationCapitalizationModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.18.1/punctuation_en_bert/93b0369b5e0d147f61895feffcbcfb88/punctuation_en_bert.nemo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"how are you\"\n",
        "result = model.add_punctuation_capitalization([text])\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnHdeaVFqMNW",
        "outputId": "92f255be-ca44-4558-8416-37a29bd4069d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-21 14:39:59 punctuation_capitalization_model:1153] Using batch size 1 for inference\n",
            "[NeMo I 2023-05-21 14:39:59 punctuation_capitalization_infer_dataset:127] Max length: 5\n",
            "[NeMo I 2023-05-21 14:39:59 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
            "[NeMo I 2023-05-21 14:39:59 data_preprocessing:406] Min: 3 |                  Max: 3 |                  Mean: 3.0 |                  Median: 3.0\n",
            "[NeMo I 2023-05-21 14:39:59 data_preprocessing:412] 75 percentile: 3.00\n",
            "[NeMo I 2023-05-21 14:39:59 data_preprocessing:413] 99 percentile: 3.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 10.14batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['How are you?']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install youtube-transcript-api\n",
        "\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64iKQQfFqoAH",
        "outputId": "280ffeae-e6fb-458b-c733-b7ae3bd37e22"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.4)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_raw = YouTubeTranscriptApi.get_transcript(\"0RYyQRQFgFk\")"
      ],
      "metadata": {
        "id": "M5KB2EeRqvH5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcript_text = ' '.join(segment['text'] for segment in transcript_raw)\n"
      ],
      "metadata": {
        "id": "NatDUzcsrUZb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = transcript_text\n",
        "result = model.add_punctuation_capitalization([text])\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC8elK4Lrbrx",
        "outputId": "d2bdf9df-e280-4b26-ee8e-4c98fed08ff0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-05-21 14:19:42 punctuation_capitalization_model:1153] Using batch size 1 for inference\n",
            "[NeMo I 2023-05-21 14:19:44 punctuation_capitalization_infer_dataset:127] Max length: 64\n",
            "[NeMo I 2023-05-21 14:19:44 data_preprocessing:404] Some stats of the lengths of the sequences:\n",
            "[NeMo I 2023-05-21 14:19:44 data_preprocessing:406] Min: 24818 |                  Max: 24818 |                  Mean: 24818.0 |                  Median: 24818.0\n",
            "[NeMo I 2023-05-21 14:19:44 data_preprocessing:412] 75 percentile: 24818.00\n",
            "[NeMo I 2023-05-21 14:19:44 data_preprocessing:413] 99 percentile: 24818.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3096/3096 [15:50<00:00,  3.26batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Welcome to the Huberman Lab podcast where we discuss science and science-based tools for everyday life. I'm Andrew Huberman and I'm a professor of neurobiology and Ophthalmology at Stanford School of Medicine. Today we are discussing mental training and visualization. Mental training and visualization is a fascinating process that has been shown over and over again in now hundreds of studies to improve our ability to learn anything. when I say anything, I mean the ability to learn music, the ability to learn and perform mathematics, the ability to learn and perform motor skills in sport in dance across essentially all domains. The other incredible thing about mental training and visualization is that, as you'll soon see, when you go into the literature, that is the scientific studies on mental training and visualization, you quickly realize that it does not take a lot of mental training and visualization in order to get better at anything. However, that mental training and visualization has to be performed in a very specific way, and today we will discuss exactly how to do mental training and visualization in the specific ways that allow it to complement the actual performance of a motor or cognitive skill, to allow you to learn more quickly, and to consolidate, that is to keep that information in mind and body so that you can perform those cognitive tasks, music tasks, motor tasks, etc, for long periods of time without ever forgetting how to do them. All of mental training and visualization relies on what I consider really the Holy Grail of our brain and nervous system, And that's neuroplasticity. Neuroplasticity is our nervous system, which of course includes the brain, the spinal cord, and all the connections between the brain and spinal cord and the organs and tissues of the body, and then all the neural connections back from the organs and tissues of the body to the brain and spinal cord. so the whole thing in both directions has the ability to change in response to experience in ways that are adaptive. That is, that allows us to do things that we could not do before, and by doing those things, or by being able to perform those mental operations, we can do better in the world that we live in. We can perform new tasks, we can think new thoughts, we can come up with novel solutions to pre-existing problems that before really vexed us, and that we couldn't overcome all of that is considered neuroplasticity. So today what I'm going to cover is a brief summary of what neuroplasticity is. That is how it occurs in the brain and body. This is extremely important to understand. If you're going to use Mental training and visualization, then I'm going to talk about what happens in our brain and body when we do mental visualization in a dedicated way. Many people have heard. Perhaps that when you imagine something happening that your brain doesn't know, the difference between that imagination of the thing happening and the real thing happening. Turns out that is not true. It is simply not true. However, there is somewhat of an equivalence between a real experience and an imagined experience, and we'll talk about the difference between those and how that can be leveraged in order to get the most out of mental training and visualization, then I will cover exactly which types of mental training and visualization work best across all domains, meaning for Music, Learning, mathematics solving puzzles, motor learning, Sports Performance, etc, etc. To really allow you a template in which you can plug in or designate what you're going to do each day for a brief period of time in order to accelerate your learning in whatever you choose, and then I'm going to go into a bit of what happens in the brains of different types of people. These different types of people that I'm referring to are people who have more or less of a natural ability to imagine things and visualize them, because it turns out that we vary tremendously from one individual to the next in terms of our ability to mentally visualize and imagine things, and our ability to get better at that over time, and the good news is anyone can get better at mental training and visualization in ways that can serve them well. I'll also briefly touch on the fact that certain people, in particular people on the autism spectrum, as well as people with synesthesias, which is the combining of different perceptual experiences, so you may be one of these people, or you may have heard of people. that, for instance, when they think of a number, they also just naturally spontaneously think of a color, and vice versa. We'll talk about how that relates to mental imagery and visualization, and the creative process and problem solving in general, And then finally what I'll do is I'll recap mental training and visualization from the standpoint of how best to apply mental training and visualization according to specific challenges, things like challenges with public speaking or challenges with sports performance or challenges with test taking Performance challenges with essentially anything that will allow you to build specific mental training and visualization practices that are brief that are supported by Neuroscience studies and that are highly effective before we begin. I'd like to emphasize that this podcast is separate from my teaching and research roles at Stanford. It is, however, part of my desire and effort to bring zero cost to Consumer information about science and science related tools to the general public. In keeping with that theme, I'd like to thank the sponsors of today's podcast. Our first sponsor is Element Element, is an electrolyte drink that has everything you need, meaning sodium magnesium and potassium, but nothing that you don't meaning no sugar. and it has the sodium magnesium potassium in the ideal ratios for hydrating and providing electrolytes to the cells and tissues of your body, So I use element in my water when I wake up. I like to hydrate right away, so I'll have an element packet in about 16 to 32 ounces of water. when I wake up, I tend to do the same while I exercise, which I typically do in the morning, sometimes in the afternoon, and I'll drink another one throughout the day. The great thing about element is also tastes terrific. I particularly like the watermelon flavor, but frankly I like all the flavors just mixed into again, about 16 to 32 ounces of water. If you'd like to try element, you can go to drink Element Lmnt.com Huberman to claim a free element sample pack with your purchase again. That's drink Element Lmnt.com Huberman, Today's episode is also brought To Us by Maui Nui venison, which I can confidently say is the most nutrient dense and delicious red meat available. Maui Nui spent nearly a decade building a Usda certified wild harvesting system to help balance deer populations on the island of Maui. The solution they built is extremely powerful because it turns the proliferation of an invasive species into a wide range of nutrient dense products from butcher Cuts, so venison steaks and ground venison, Etc, to organ Meats, bone broth and jerky, I really love all of their products, so I love the venison steaks and the ground venison and their bone broth is fantastic. It has an unmatched 25 grams of protein per 100 calories, so it's very high density, High, high quality protein per calorie, So if you'd like to try Maui Nui venison, go to Maui Nui Venison.com Huberman to get 20 off your first order again. That's Maui News Venison.com Huberman to get 20 off your first order. Today's episode is also brought To Us by eight sleep. Eight sleep makes Smart mattress covers with cooling, Heating and sleep tracking capacity. I've talked many times before on this podcast about the fact that sleep is the foundation of mental health, physical health and performance. Now, one of the key features to getting a good night's sleep is making sure that you get the temperature of the environment you sleep in right. That's because, in order to fall asleep and stay deeply asleep throughout the night, your core body temperature needs to drop by about one to three degrees. Conversely, in order to wake up in the morning feeling refreshed and ready to go, your core body temperature needs to go up by about one to three degrees. Of course, you can adjust the temperature of the room that you sleep in. I do hope that people are doing that, but adjusting the temperature of your mattress and your direct sleeping environment is also key, and with the eight sleep it makes it very easy to program the temperature of that mattress in a sleep environment not just throughout the night, but for specific phases of the night. I start sleeping on an eight-sleep mattress cover well over a year ago, and it's the best sleep that I've ever had. If you'd like to try eight sleep, you can go do Eightsleep.com Huberman and check out the Pod Pro cover and save 150 at checkout. Eight sleep Currently ships in the Usa, Canada, Uk, select countries in the Eu and Australia again. That's Eightsleep.com Huberman To save 150 at checkout. Let's talk about mental training and visualization Now, perhaps surprisingly, mental training Visualization has been studied since the late 1800s. It's actually a paper published in 1880 by Galton called the statistics of mental imagery So long ago people were quantifying and trying to understand how is it that people come up with mental images and how they can apply that to learning things more quickly and more stably over time. Now, as I mentioned earlier, mental training and visualization relies on a process that we call neuroplasticity. Neuroplasticity is a term that many people have heard and encompasses many different things. So broadly speaking, neuroplasticity includes developmental plasticity, which is the sort of plasticity that occurs between about birth and age 25, and that can be summarized very easily as passive plasticity. in other words, the sorts of changes that happen in one's nervous system simply by engaging in the world and experiencing Life as a child, as a young adult as an adolescent and as a 22, 23, 24 year old, Etc. Now, of course, of course, of course, it is not the case that on your 25th birthday you close out passive developmental plasticity and start engaging in the other type of neuroplasticity, which is adult neuroplasticity. It's a gradual tapering off of Developmental plasticity that occurs between age zero and 25, and for some people might occur somewhere around 26. For other people around 23. When we say 25, we're really just talking about the average age in which passive plasticity tapers off. However, starting fairly early in adolescence and extending all the way out into one's 80s or 90s or hundreds. Should one live that long, is the other form of neuroplasticity, which is adult neuroplasticity, Adult neuroplasticity is very different than developmental plasticity, because it is the sort of plasticity that one can direct towards one's own specific desired learning. So if we wanted to get a little bit technical here for sake of clarity, not for sake of confusion, we would say adult plasticity is really about self-directed adaptive plasticity and the reason we call it that, as opposed to something else, where simply adult plasticity is that there are many different forms of neuroplasticy, there is for instance maladaptive neuroplasticity that occurs if one gets a really hard head hit and concussion, there will be changes to the brain and nervous system, but those changes to the brain and nervous system do not allow it to perform better. In fact, it often impairs the brain and nervous system's ability to function, and therefore is maladaptive. so I don't want to get overly wordy with a number of different terms here, but I do think it's important to understand that we have developmental plasticity again, in which the brain and nervous system changes simply in response to experiencing specific things for better or worse, and there's adult self-directed adaptive plasticity, in which one can direct specific changes in terms of learning things cognitively or learning things in terms of motor function, so sport Dance, Etc, or a combination of the two. Now, just to really clarify what I mean by developmental versus self-directed adaptive plasticity, I mentioned that self-directed adaptive plasticity actually can start in adolescence right, even though there's ongoing developmental plasticity. I mean, let's be really direct. the brain of a 14 year old is very different than the brain of that same individual when that person is 21. Because there's ongoing developmental plasticy, However, starting at about adolescence, we can all start to decide what it is that we want to learn and engage in self-directed adaptive plasticity. Now the way to engage self-directed adaptive plasticity, regardless of whether or not you're a 13 year old, 14 year old or you're a 90 year old, or anywhere in between, is that it requires two things. The first thing it requires is focused dedicated attention to the thing that you're trying to learn. That's the first step and that actually triggers a number of different chemical and electrical prostheses in the brain that are often associated with agitation and frustration. Believe it or not, the agitation and frustration is a reflection of the release of specific chemicals, in particular, Norepinephrine, and epinephrine, also called noradrenaline and adrenaline in the brain and body, that creates this discomfort and this heightened level of alertness and attention that many of us don't like and tend to back away from, But it is exactly that chemical, or I should say, neurochemical milieu, which signals to the neurons, the nerve cells in the brain and elsewhere in the body that something needs to change. Because if you think about it, if you can do something perfectly, or if you try and do something, and it doesn't cause any neurochemical change in your brain and body. Well then there's no reason for your brain and its connections with the body to change in any particular way. Okay, so you need focused dedicated attention to the thing that you're trying to learn. It's off often accompanied by agitation frustration. Etc. So that's perfectly normal. In fact, that's a signal that things are going right, Meaning they're headed towards learning. But there's a second component that's really required for self-directed adaptive plasticity, and that's periods of deep rest, in particular, a good night's sleep in particular on the night that follows that focused attention to the thing you're trying to learn. There are now hundreds of studies in both animal models and in humans, showing that it is really during sleep and other states of deep relaxation, things like meditation and non-sleep deep rest, which I've talked about before on this podcast, but really during our main night of sleep, that the rewiring of neural connections, that is the actual neuroplasticity, takes place. So the verb neuroplasticity, the rearrangement of connections between neurons really occurs during sleep, in particular on the first night following an attempt to learn something through this focused attention. now, developmental plasticity, which is passive also requires good sleep sleep. It's slightly different, or frankly, it's a lot different in terms of the underlying mechanisms than self-directed adaptive plasticity, But because today we're mainly talking about how to learn faster through mental training and visualization, and that really Maps more closely onto self-directed adaptive plasticity. I just really want to emphasize this two-step process. There has to be focused dedicated attention, and then there needs to be sleep, and in particular sleep on the first night following that training. Now, should you have the unfortunate experience of getting woken up in the middle of the night following trying to learn something, or should you simply not be able to sleep for whatever reason on the night following a bout of learning or an attempt to learn. Do not despair, because it turns out that there are what are called second and third night effects. Also, once you sleep, you will learn those neuroplastic events, the reordering of connections that we call synapses, and the changes that occur in neural circuits that reflects what we call self-directed adaptive plasticity. that still will occur Her. But ideally you got a great night's sleep on the first night following, trying to learn, and the second night and the third and so on, and so on. Now there are a few other things that are critical to understand about self-directed adaptive plasticity, that will become especially important when thinking about protocols for developing the ideal mental training and visualization process for you. And that is that there are different forms of plasticity that occur between neurons. Although the two main forms are what are called long-term potentiation and long-term depression. I just want to cue up right now that the word depression is a very loaded word because the moment people hear the word depression, they think, Oh, no, that's bad. but in the case of neuroplasticity, long-term depression is simply a change in the connections between neurons and the excitability between neurons. that in many ways can be excellent for learning things in particular motor skills, and we'll get into this in more detail in a little bit, but it turns out that a lot of our ability to get better at some sort of motor skill involves this thing that we call long-term depression, and that's because much of what is happening when we learn a new motor skill is that we are depressing or suppressing specific actions in order to generate a very specific coordinated action. Some of the best examples of long-term depression can actually be borrowed from developmental plasticity. So for instance, if you've ever sat across from an infant who is trying to eat their meal, so imagine a one and a half year old or a two-year-old trying to eat some noodles or some soup or any kind of baby, suitable food with a spoon, And they're holding the spoon, or they're trying to hold the spoon. What you'll notice is that their motor movements are terribly uncoordinated. They often will take that spoon to their cheek or to their eye to their head. We've all seen these very amusing photos of babies with bowls of food on their head, or with food all over their face, or just everywhere. It's appears that they're basically getting the food everywhere except where it's supposed to go, which is in their mouth, and that's because their motor movements are not very well coordinated at that age, and they're not very well coordinated, not because they lack sufficient numbers of neural connection synapses between neurons, but rather because they have too many connections between too many different neurons, the neural circuits that control very dedicated coordinated movement are not there yet. Instead too many neurons are connected to too many other neurons. and so they can't generate the precise movements that are required in order to get that spoon to their mouth. Now, over time they get better at moving the utensil to their mouth such that hopefully by about age five or six they are eating, you know in a relatively cleaner way, and hopefully by time they're 10 or 11 or 12, they're getting the food into their mouth and not all over their face. People learn this to varying degrees. All you have to do is go to a restaurant and watch how people eat. Um, and you will see a vast variation in people's coordinated movements with utensils, but in general there's a theme. The younger the person, the more uncoordinated their movement of utensils, and as they get older, the more coordinated now. of course, in people that are very old, they have challenges moving objects and their limbs in very smooth ways, and that has to do with a topic that we'll get into when we talk about age-related cognitive decline and motor related dementias, But for sake of today's discussion, if you just want to think about what happens with long-term depression and the development of a motor skill both as a baby as an adolescent and as an adult, when you're trying to learn a new motor skill is that you are eliminating incorrect movements, and when you are eliminating incorrect movements, to arrive at only the correct movements in a very reflexive and repeated way. So think your golf swing your tennis serve. I think serving a volleyball. Think a child learning to crawl and then walk, think a child learning to eat with utensils. An example I gave before. What's happening in all of those cases is that Yes, certain Connections in the brain are being strengthened, or what we call potentiated. they are undergoing long-term potentiation. The so-called quote, Unquote fire together, wired together Mantra that was popularized by the great neurobiologist, Dr. Carla Schatz, my colleague at Stanford, but, in addition to that long-term depression, the quieting or the silencing of specific synapses, that is, connections between neurons, is absolutely critical for motor skill learning. so we have Ltp long-term potentiation and Ltd, long-term depression is every bit as important as Ltp long-term potentiation for getting better at some sort of motor skill, and indeed it getting better at some sort of cognitive skill. Now as we hear this, this should be intuitive to all of us if you look at somebody's attempt to learn a particular dance step, or at somebody's attempt to do a tennis serve the first time. it's all over the place. Now, it's not perhaps all over the place in that they're doing a jumping jack while trying to serve the tennis ball, But they're generally arcing the racket too widely on one trial, and then they're arcing it too close to their body on the next trial, so if we were to draw a line over each one of those trials, we would see that there were lines everywhere over time, Whereas once they quote unquote, affect the tennis serve. It's going to be line drawn directly over line, draw, and directly over line, Meaning the Arc of that tennis serve is going to be very restricted, and that without question has reflected the removal or the quieting of particular synapses, connections between neurons in the brain and body to allow that very narrow, coordinated and directed movement. The same is true for learning anything in the cognitive domain. Meaning, if you are to learn a language, it is not, of course the case that you know every word in that language, and then you simply remove certain words and arrive at the correct sentence structure that you're trying to achieve, But rather you have to suppress your native language, or if you're a young child, you have to suppress the generation of just kind of random babbling sounds. Turns out babbling isn't random at all, but the point is that you have to suppress the enunciation of particular sounds and direct the pronunciation of other sounds in order to generate that new language or your ability to speak at all. Okay, so we can really think about neuroplasticity as both a building up process in which you increase connection, so-called long-term potentiation, and a sculpting down or a removal of connections process that we're going to call long-term depression now, I have to acknowledge that, of course there are other forms of neuroplasticity, too. I know there are probably some aficionados listening to this who will be perhaps shouting back at. Uh, whatever device my voice is coming out of Wait. what about Spike timing dependent plasticity, or what about paired pulse facilitation? Yes, yes, and yes. there are multiple forms of communication between neurons that can strengthen those connections or weaken those connections. But for today's discussion we just broadly want to think about long-term potentiation and long-term depression because it captures the two most important themes related to mental training and visualization, which is that when we perform a given cognitive or physical task in the real world, so we actually try the dance step or the tennis serve, or when we actually try a math problem, or we try and learn some specific knowledge and write it down and remember it. That is engaging particular neurons. right, they're firing. They're releasing chemicals, but it is also actively suppressing the activity of other neurons and we are always completely unaware of the ways in which our brain is suppressing certain activity. Okay, so today we have to keep in mind that where there is strengthening of connections, there is also weakening of connections. and when it comes to mental training and visualization, and here's the really key point with mental training and visualization, you are capturing both processes, both the potentiation that is the building up and strengthening of connections and the weakening of the connections that are inappropriate for the thing you're trying to learn. And there are different aspects of mental training and visualization protocols that really harness the potentiation versus the depression aspect, and today we will cover mental training and visualization protocols that capture both the potentiation and the depression aspect of neuroplasticity, and in that way serve as an augment that is a complement to the actual real world, cognitive and physical training that you're doing. Because I'll just give this away right now. Turns out that mental training and visualization is not a replacement for real-world cognitive or motor Behavior again. mental training and visualization cannot replace real world execution of cognitive tasks or of motor tasks. If you want to learn. However, mental training and visualization can, and has been shown to be effective for greatly enhancing the speed at which you learn and the stability of that learning over time. Okay, so let's take a second and really think about what's happening in the brain and body when we do mental training or visualization. In fact, we can do a little experiment right now. that is not unlike many of the classic experiments, looking at what's happening in the brain and body through mental training and visualization, in which I just ask you to close your eyes and imagine a yellow Cube. Okay, and next to that yellow cube is a red rose. And perhaps I also ask you to float or fly up above the cube and the rows and look at them from the top down. And then I tell you to fly back around and land behind those and look at them from the perspective of behind that yellow Cube and that red rose. Okay, Now what the data tell us is that most people will be able to do that. Most of you will be able to do that to some degree or another, regardless of your attention span. Whether or not you have ADHD or not, most of you will be able to do that to some degree or another. We also know from neuroimaging studies in which people are placed into a functional Magnetic Resonance imaging scanner that during the sort of visualization you just did, or that I described that your visual cortex and Associated areas quote, Unquote light up. they become very active in similar but not identical ways to how they would light up and be activated. Were you to actually look at a yellow cube and a red rose on a screen, and perhaps Fly Above them virtually, of course, and land behind them virtually, of course, or if you were to actually look at a yellow Cube and red rose in the real world right in front of you on a table, then you know, get up on your tippy toes and look down on them from the top, and then walk around the table and look at them from the other side. So there is some degree of what we call perceptual equivalence between real world experiences, digital experiences and imagined meaning with our eyes closed just in our Mind's Eye experiences. This is true not just of vision and what we call the visual domain, but also the auditory domain. Okay, so for instance, I could play for you a short motif of a song. Let's just pick something that I think most people know. Goodness. I'm a terrible musician and even worse, um singer. But let's just take the the opening to Ac Dc's Back in Black right. I think I can do that when it's like [Music] Okay, Got it. That's the actual sound, although admittedly a dreadful version of of the great Ac Dc song Back in Black, But now I ask you to close your eyes, or we could keep them open and just imagine that, Okay, or for instance, I place you in a quiet room, so you close your eyes and ask you to imagine the opening to Ac Dc's Black and Black, but ask you to pause it halfway through. What you would find again is that most people, somewhere between 90 and 95 of people would be able to do all the sorts of things I described, right Cube and Rose Acd Back in Black. Even a somatosensory task. I imagine you need to imagine what it's like to touch felt or to touch chinchilla hair or something like that, H Chinchilla's hair, Ideally a live chinchilla sitting still. those little critters move really really fast, but they have very very soft hair. High hair density so soft. Okay, most people can do that. About five to fifteen percent of people are less able to do that, and there's a small percentage of people in that five to fifteen percent that simply cannot do it at all. that just cannot visualize. Well, we'll talk later about these people. They have what's called aphantasia, an inability to mentally visualize, But most people are actually pretty good at visualizing things when they are told what to visualize. And and this is a really key point, And if what they are told to visualize is very simple, and the whole visualization is quite brief, lasting on the order of about 15 seconds to generate the visualization in the auditory or in the visual aspect of one's Minds, I, or ear, if you will, and if it's repeated over and over, what's far harder for everybody to do, and in fact, what most people simply cannot do is Imagine long, extended scenes and stories in their mind that go on for minutes and minutes that involve a lot of different sensory stimuli. This is a really key point. in fact, as we start to hone in on ideal mental training and visualization protocols, I'd like to establish this as the first principle of mental training and visualization, which is that, if you are going to use Mental training and visualization to its best effect in order to engage in neuroplasticity and learning, you need to keep those visualizations quite brief, really, on the order of about 15 to 20 seconds or so, and pretty darn sparse meaning, not including a lot of elaborate visualization, not including a lot of sequences of motor steps. What I mean are motor sequences if you're trying to learn something in terms of physical movement or visual sequences or auditory sequences. If you're trying to learn things in terms of music or Dance, Etc, that can be completed and repeated in 15 seconds or less Now later, I'll give you a couple of specific examples, but if you want to use Mental training and visualization, understand this is the key first principle. They have to be very short visualizations that you can repeat over and over and over again with a high degree of accuracy, so you don't want to embark on a mental training and visualization Paradigm in which it involves a lot of elaborate stimuli and you have to think really hard and work really hard, even if you're in that category of people who can do mental visualization pretty naturally and easily Now if you're somebody who can't do mental visualization, In fact, if you're somebody who has full-blown aphantasia or the inability to mentally visualize, well, then it's especially important that you make those mental trainings and visualizations really brief and very very simple. I'd like to take a quick break and acknowledge one of our sponsors, Athletic greens, Athletic greens, now called Ag1, is a vitamin mineral probiotic drink that covers all of your foundational nutritional needs. I've been taking athletic greens since 2012, so I'm delighted that they're sponsoring podcast. The reason I started taking athletic greens, and the reason I still take athletic greens once are usually twice a day is that it gets to be the probiotics that I need for gut health. Our gut is very important. It's populated by gut microbiota that communicate with the brain, the immune system, and basically all the biological systems of our body to strongly impact our immediate and long-term health, And those probiotics and athletic greens are optimal and vital for microbiotic health. In addition, athletic greens contains a number of adaptogens, vitamins and minerals that make sure that all of my foundational nutritional needs are met and it tastes great. If you'd like to try athletic greens, you can go to Athleticgreens.com Huberman and they'll give you five free travel packs that make it really easy to mix up athletic greens while you're on the road in the car on the plane, Etc, and they'll give you a year's supply of vitamin D3k2 again. That's Athleticgreens.com Huberman to get the five free travel packs and the year supply of vitamin D3 K2 Now, in order to develop the best mental training and visualization protocols for you, let's go a little bit deeper into what the research says about mental visualization. Now, the classic work on mental visualization really hinges on a number of different researchers and their work. but in particular Roger Shepard, who did this work at Stanford, and Stephen Coslin, who's now at Harvard. Of course, others in the field. But it's really the work of Shepherd, The lay, the foundation for our understanding of what happens in the brain when we mentally visualize something. Shepard did these incredible experiments in which he had students mentally visualize simple objects like a square like a triangle, and he measured how long it took them to do that. Now, of course, at the time when he did these experiments, there were no sophisticated brain Imaging devices and machines like Fmri. However, everything I'm about to describe has been later confirmed using things like Fmri. What Shepard did and what he found is that if people were told to visualize very simple objects, they did it pretty quickly. However, if they were told to visualize more complex objects, or importantly to rotate those objects in their Mind's Eye, well, then it took longer for them to perform those mental visualizations. Now many of you might think duh. if I have to just imagine a triangle or a cube, that's going to be very easy and very fast. Whereas, if I have to rotate that triangular cube in my mind's eye, that's going to take more time. and indeed, that is somewhat of a duh Except, and this is so very important except that what Shepard and his colleagues found is that how long it takes somebody to generate and rotate a given visual image scales directly with the complexity of that image. In fact, Causalin did some experiments, I think, illustrate this even better. and here's the experiment. I love this experiment. I think you'll love it too, because it illustrates something so fundamentally important about how our brains work, not just for sake of mental training and visualization, which is how our brains work at all. He showed people a picture of a map, So a map drawn on a piece of paper. This was a map of an island. It included things like a loading dock for some boats. It had a location for getting food on the island. At some trees, Add some other small landmarks drawn out, and people looked at this and memorized it, or in other experiments. They just had people imagine this island and the location of these different landmarks on the island. so it didn't really matter which, but then he had people imagine moving or walking from one location on the island to another so they'd say Okay, you're at the loading dock. Now move to the restaurant. Okay, you're at the restaurant. Now move to the palm tree. You're on the North Shore of the island. Now go around the side of the island clockwise to arrive at the bay on the southwest corner. This sort of thing. What Causlin found was absolutely incredible. What he found was that the amount of time that it takes people to move from one location on the map to another scaled linearly directly with the actual physical location between those objects on the map. So for those of you that can understand or into the importance of what Shepard and Causlin showed great, I'm guessing, however, that for most people out there you're still grasping like Okay. Interesting, you know how things happen in the real world, dictates how they happen in our mind's eye, but I want to make sure that I really nail home the importance of this for everybody. The importance of this is that when we look at something in the real world, so if I look at the pen in front of me, I'm holding up my pen for those of you that are listening. Just holding up my pen in front of me. I move it to the right and back and forth. What's happening is I'm activating or I'm triggering the electrical activity of neurons, which we can think of kind of as pixels in my eye. Okay, so it's leftward to right word motion for me and back and forth and those are getting activated and they're sending signals up to my visual cortex and that information is processed at a given speed. What the visualization experiments that Shepard and Koslin and others did show is that the processing speed of imagined experiences is exactly the same as the processing speed of real experiences, and the spatial relationship between imagined and real experiences is exactly the same as well, put simply, when we imagine something in our mind's eye or mind's ear, we are Imagining the real thing happening, and when I say the real thing, it's not the obvious real thing. Of course, if you're imagining something, that's the thing you're imagining. What I mean is that your brain at the level of neurons is behaving exactly the same way, And this needn't have been the case. Okay. there could have been a result. for instance, that if people were asked to visualize a cube and rotate it from you know, flip it from top to bottom. Okay, so put the top that's upward on a table, now down on the table, and so forth, or to migrate around the island, you know counterclockwise going from, you know, the northern coast all the way down to the southern coast, clockwise, and then back up to the northern coast. That they could have just done it really quickly like all in one second. But that's not what happens. They always match the speed at which they do things in their Mind's Eye to the same speed that they do them in the real world. So in telling you this, what I'm saying is that mental visualization at the neural level is identical to real world events, So when you've heard that when we imagine something, it's identical in terms of our brain's experience of it, and our bodies experience of it as when we actually experience something that is true at the neural level. However, when it comes to learning and improving performance in the cognitive or physical domain, they are not equivalent. So this is the second principle of of mental training and visualization. as you recall, The first principle of mental training and visualization was that in order to make it effective, it needs to be very brief and very simple and repeated over and over again. The second principle of mental training and visualization is that while yes, mental training and visualization recaptures the same patterns of neural firing in the exact same ways as real world behavior and thinking. it is not as effective as real world behavior and thinking. In other words, if you want to learn something, the ideal situation is to combine real training in the physical world with mental training, and I'll talk about exactly how to do that, and in what ratios a little bit later Now there's a really incredible set of experiments that illustrate why it is that mental training and visualization can be extremely effective, but that it's always going to be most effective when combined with real world training and experiences. The experiments that I'm talking about involve the use of what are called bistable images or impossible figures. Now, some of you are probably familiar with impossible figures. These are figures or objects that when you look at them, they have these odd features like you're not sure where they stop and where they start. where they end. One good example would be the so-called Mobius strip. The Mobius strip is literally a strip or a line that is contiguous. It goes up and it loops around, and then it curves around, and then it goes back and and it just continues and continues. and when you look at it, you can never really tell where it starts and where it stops, because it doesn't have any of the features that allow you to see what's the front and what's the back in any kind of stable way. Another example of an impossible figure would be, you know, a little set of Cubes that look like they're coming out toward you, maybe with a little Bend in them going up at a right angle. perhaps, But then if you look at it a little bit longer, that little piece that's facing up looks like it's in front and you can't really tell what's in front and what's in back. And so it's called an impossible figure because you don't really know how to frame it in your mind to tell what's closer to you and what's further apart. Bi-stable images are somewhat similar, although different in the sense that they typically are simple Silhouettes, So for instance, the faces, vases by stable image is perhaps the most famous of these. Where you look at this image, it's very simple and it looks like two vases, but then you look at it a little bit longer, and you realize that you're looking at the side angle or the profile of two faces looking at one another, and when you see those two faces looking at one another, you can't see the vases at the same time, But then if you decide to see the vases again, you can see the vases again, but the faces disappear, so it's bi-stable meaning that you can't see the faces and bases at the same time, and impossible figures and bi-stable images are capturing the fact that your visual cortex and some of the associated areas that compute visual scenes in your world are essentially trying to recreate whatever it is that's out in front of them, And that's effectively what your visual system does. It's very good at recreating visual images in your brain in your mind's eye, because if you think about it, even with your eyes open, your brain is just creating an abstract representation of what it thinks is out there, but that when it comes to assigning an identity to something like, Oh, that's a face or Oh, that's a vase that is constrained by different neural circuits by different areas of the brain, and somehow those circuits can't be co-active We cannot see the faces and the vases at exactly the same time. We can switch back and forth really quickly, just as we can switch back and forth really quickly when we're looking at the impossible figure and think Okay, that's the front of it. That's the back. No, wait, that's the back. that's the front end. It's going back and forth, but we can't see them both at the same time. No one can see them both at the same time. Okay, we know this from brain Imaging studies. Now impossible figures in bi-stable images can be seen right. You could look them up right now on your phone or computer, or I could show you pictures of them on paper right in front of you, and you can do these sorts of perceptual experiments of telling people, look at the face, look at the vase, look at the front of the cube, and I'll make it at the back of the cube, and they can do this somewhat deliberately. However, and this is, I think so, very interesting to understanding how mental training and visualization does, and does not support Real World Learning. If you try to imagine a bi-stable image, you can't do it. In fact, no one can do it until they do something else. Okay, So for those of you saying wait, I can do it. I can do faces, faces in my mind's eye. I promise you that the neuroimaging disputes your belief, Okay, and supports the idea that we can see real world by stable images. We can see real world impossible figures. but when we try and imagine those in our Mind's Eye, we simply can't do it. We can't do the perceptual shift in our Mind's Eye. We can't switch back and forth between faces and vases, However, and I just have to chuckle because I think these experiments are so clever. If I have you trace or Draw with a pen on a piece of paper, an impossible figure or the faces, vases by stable image, And then I ask you to imagine that bi-stable image or impossible figure, and to switch back and forth you were able to do it. So what that illustrates is that it's the combination of imagined and real world experiences, real motor movements, real perceptual experiences combined with motor movements combined with what you imagine in your mind's eye, that really gives you the most depth and flexibility over your mental visualization, and in doing so we can really stamp down a third principle of mental training and visualization, which is that your mental training and visualization will be far more effective if you are performing the exact same or very similar mental and physical tasks in the real world. Okay, so first principle is mental training and visualization needs to be simple and brief and repeated. Second is that mental training and visualization is not a replacement for real world motor training or cognitive training. It's an augment. It's an addition that can really help. And the third principle of mental training and visualization is that you need to combine mental training and visualization with real world behaviors and experiences that are very very similar now as a brief, but I think really relevant. Aside. one of the things that also makes mental training and visualization more effective is when we assign cognitive labels to what's going on when we visualize. so what I mean is that people are much better at manipulating faces and bases in their Mind's Eye. Of course, only once they've drawn them out physically with their hand, as I mentioned before, then they are manipulating abstract objects like impossible figures. in part, because by labeling them faces and vases, people are able to capture a lot of other neural Machinery that's related to faces and bases. In fact, we have entire brain areas on both sides of the brain devoted to the processing of faces. They're called fusiform face area. We have other areas in our brain that are involved in processing of 3D objects. but faces are of particular value. There's a. There's a value to understanding what a face is, as opposed to a non-face and there's a value to understanding what a particular face is. In fact, the simplest way to put this is that the human brain is in many ways a face recognition and expression of faces recognition machine. It of course, does other things, but it is exceptionally good at that, unless you're in a profession in which the relationships between 3D objects and your ability to manipulate them is exceedingly important. You're not going to have a lot of neural real estate specifically devoted to that. Some people will be better at it. some people will be worse. but when it comes to faces, unless you have a condition like prophetsagnosia, which is an inability to recognize, say famous faces and distinguish them from non-famous phases, or if you have some sort of face recognition deficit, which about anywhere from one, perhaps to three percent of people out there have. Yeah, They're just terrible recognizing faces, and by the way, there's about half a percent of people out there that are what are called super recognizers that can recognize faces in a large crowd. They can recognize specific faces even from just partial profiles. By the way, these people are extremely valuable to security agencies and security agencies are very good at finding these people. Um, machines are quickly getting better, or at least as good as super recognizers, but the best super recognizers are still better than the best Ai and machine algorithms out there, But the point is that in your mind's eye you are better able to manipulate specific objects or to see things more clearly and with more specificity, when it has a label that you recognize from your real world experience, as opposed to abstract or fictional labels. Okay again, Stamping home the idea that what you experience in the real world really serves to support your mental imagery, and therefore the key importance of experiencing and doing things in the real world, and supporting that with mental training and visualization, and not just relying on mental training and visualization and the tangent here. That's a little bit of fun and I don't think we've ever talked about before On this podcast is that of Ufos, Unidentified flying objects. You know, there's a lot of people out there who think that they've seen Ufos. I guess technically they have because a Ufos and identified flying object, And if it's unidentified at least to them, then it is indeed a Ufo. I guess the question is whether or not uh, or the dispute rather is whether or not those Ufos are actually flown by aliens or controlled by aliens. I think that's where the dispute lies, but you can imagine how if somebody sees an object in their environment and decides Ah, that's a Ufo. Okay, Remember these faces, faces or these impossible figures. If they say, Oh, that thing is a Ufo, as opposed to something else they see. In other words, the face, not the vase. Well that stamps it down as a memory in their visual system and related systems, and then in their Mind's Eye they are seeing the Ufo. They're not seeing the other thing. that it could possibly be okay. so it's stamped down a very specific memory. So the point here is that mental training and visualization relies on not just the physical Contours and the exact spatial profiles and the speed of movement of particular things that we experience in the real world. It also heavily depends on the cognitive labels and the decisions we make about the things that we see, and this will become very important as we build up toward our fourth principle of mental training and visualization, which is that our cognitive labels, that is what we decide is happening when we do mental training and visualization, turns out to be very important. Now this is not simply to say that you can decide. Okay. I want to learn how to play piano. And so I'm going to tell myself that a particular chord I imagine in my mind's eye is identical to the real world chord. Just because I decide it is, The brain doesn't work that way. It's not possible to just lie to yourself and learn better as a consequence of the lies you tell yourself. However, what this tells us is that it is very, very important that your mental training and visualization accurately recapitulate the real world training that you're doing. so. if we are going to stamp down a fourth principle of effective mental training and visualization based on what we know from the scientific literature, is that your mental training and visualization should assign labels to what you're doing that can be matched to real world training and experiences. Now these can be somewhat abstract, so for instance, if you're trying to learn a particular aspect of the golf swing, okay, so let's say that you're working on your golf swing. Seems to be there are a lot of people out there working on their golf swing, and you're going to do some mental training and visualization in order to improve your golf swing. We already know again. Let's just March through them that your mental training and visualization needs to be brief and simple. It needs to be the same, or in fact, it will be. We can say the same as your real world golf swing. In other words, it will take you exactly the same amount of time to perform that golf swing in your mind's eye as it would in the real world, incredible, right again, Something that maybe is taking a little bit of time to sink in. but once it does, you're gonna be like Wow, the brain is really an incredible machine, and that third principle that you still have to do golf swings in the real world, in addition to the mental training of golf swings, and fourth, that if you want that mental training and visualization to really improve your golf swing, you're going to have to name or apply an identity to the specific golf swing or aspect of the golf swing that you're practicing. So this could be abstract. You could call it mental training and visualization of golf swing 1a, And you can imagine your mind's eye. You know, the perfect golf swing over and over and over and over, but then when you're in the real world, you're also going to have to call that either out loud or just to yourself. Golf swing 1a, Okay, as opposed to a putt, which might be 1b, So naming and giving an identity to a real world skill and applying the same name or identity to the mental version of that, the visualization of that can enhance the mental training and visualization in significant ways. So when we apply identities or names to these mental trainings and visualizations and again provide that they are brief and repeated, and so on, we greatly enhance the amount of neural Machinery in the brain and body that we are able to recruit when we go to perform those real world golf swings and golf putts, And here, just replace golf swing and golf butt with anything that you're trying to learn. You're able to recruit a lot more neural machinery and greatly increase the probability of proper execution. So before we go any further, I want to share with you a couple of incredible aspects of mental visualization that really can be harnessed and applied toward mental training and visualization. Some of these were done by Roger Shepard and his graduate students. in postdocs. Some were done by Steve Costlin, and by others, What these experiments really show is that mental training and visualization is capturing many, many of the exact same features of real world behavior and perceptions. not all of them but many of them. So for instance, if I tell you to close your eyes and imagine a ceiling that has tiles that are black and white, checkered tiles. you know, one black tile, one white tile. For instance, we know, based on experiments where we measure eye movements Behind Closed eyelids, is that people tend to move their eyes up when they are imagining things above them, such as the ceiling. Whereas, if I tell you to imagine things down on the floor like you're taking a hike and you're looking for rattlesnakes. Actually, just recently I experienced because it's spring here in California. rattlesnake along a hiking trail. It's really quite beautiful. although I have to confess, I enjoyed keeping my distance. I don't like snakes. Uh very much. I don't dislike snakes, but I prefer not to interact with them unless I have to. If I have you, imagine that rattlesnake, depending on your relationship or thoughts about rattlesnakes, number of things will happen in your brain, of course, activation in the limbic system or not, for instance. But what I know is that, regardless of how you feel about snakes, most of you will move your eyes down when imagining a snake. Okay, it might be subtle. it might be fast, but statistically that result shows up as opposed to when I imagine, or I ask you to imagine something above you. You tend to move your eyes up in addition to that. If I tell you, for instance, to imagine an elephant and a mouse next to one another, you presumably have some real world understanding about the relative sizes of elephants versus mice. Elephants generally are bigger than mice. Thank goodness, mice are smaller than elephants. If I ask you to tell me about the details of that Mouse's face, so for instance, can you see its whiskers? The processing time required for you to do that is much longer than the processing time required. If I say, tell me what the position of that elephant's trunk is now. why would that be so okay? The position of the elephant's trunk wasn't something that I told you. It wasn't dictated by me. It's in your mind's eye. Maybe you don't even know and you have to go searching for it. But what we do know is that if I tell you to look at a small object in your mind's eye versus a larger object, so for instance, the mouse versus the elephant, it takes longer for you to do that. In other words, just as with the map experiment, the distance between things on a map is conserved in your mind's eye, As a linear relationship takes longer to go far distances between things on a map in your mind than it does to go shorter distances. It's also the case that it takes you longer to look at the details of a small object versus a large object, Because, why? because you are zooming in in your mind's eye again, all of which speaks to the equivalence of mental imagery with real world imagery and perception. And as I mentioned earlier, and as we'll see in a moment, this also extends into the motor domain. It takes you longer to perform Complex Motor sequences in your mind's eye than it does simple motor sequences, just as it would in the real world, And if you're saying, of course, of course, of course, well then great, then we've really underscored the point, which is that when you imagine things, it is not exactly the same, but it is very, very much the same as actually doing or perceiving those things in the real world, And the fifth principle of effective mental training and visualization is this notion of equivalence of mental imagery versus Real World perception and behavior. These are the experiments as you recall, where if people are told to look for clouds in their mental visualization, they tend to look up, or if they're looking for something on the floor, they tend to look down even Behind Closed eyelids. Now this can be applied toward building an especially effective mental training and visualization protocol if you deliberately move your eyes in the direction of the thing or things that you are trying to recapitulate in your mind in your visualization, that is, you don't necessarily have to include this step, but mental training and visualization is going to be more effective if you do, because with consciously generated eye movements again, even Behind Closed eyelids, you are bringing about more of the neural circuitry that one would experience if you were to perform that particular cognitive task or motor task in the real world, which, as I mentioned before, in principle number three, you need to be doing anyway separately from your rental training and visualization. So what we're talking about here is thus far five principles of mental training and visualization that are well established from the scientific research literature. In fact, I haven't mentioned this quite yet, and I'll refer to some other references, but there's a wonderful systematic review of a large number of studies that have looked at mental training and visualization. What's effective? What's less effective across a bunch of different disciplines that include education, medicine, music, psychology and sports. We will provide a link to this paper in the show note captions, But the title of the paper is Best practice for motor imagery, A systematic literature view on motor imagery training elements in five different disciplines, as the title suggests, it's mainly for motor imagery training, but it extends into music, which of course involves motor training and execution, but as well as education, this review establishes a number of different important things. I'm going to read off some of the key or highlight takeaways. For instance, I described principle one of effective mental training and visualization, which is that the visualization be brief and it be simple and it be repeated. You may ask how many times that very brief 5 to 15 second exercise of going through some routine should be repeated. Well, different Studies have used different ranges of. Let's call them repetitions in a given training session, But the number that seems to be most effective is somewhere between 50 and 75 repeats per session. That brings about the question of how long one should rest between each repeat. This gets a little tricky depending on what you're trying to do. Remember that we have this threshold of about 15 seconds for completion of the entire motor sequence. Let's say what you're trying to do. Like a golf swing. takes you five seconds to imagine in your mind's eye from the point where you, let's just say have the ball on the tee. You bring the the golf club up. You might reposition your your feet just a little bit. you know that kind of a little wiggle that golf golfers do, and then the swing. If that whole thing takes five seconds in your mind's eye and roughly five seconds in the real world. Well then you'd be able to repeat it, of course, three times in 15 seconds. That would be one repetition, even though you're doing it three times. so it's one 15 second Epoch, as it's sometimes called Epoch Epoch, and then you would rest for an approximately equivalent amount of time, 15 seconds or so, and then repeat and the rest 15 seconds or so, and then repeat rest 15 seconds, and then repeat again, three golf swings within that 15 seconds rest 15 seconds, three golf swings within that 15 seconds rest 15 seconds. Truth told, these epochs and these rest periods do not need to be exact. You could imagine, for instance that you get three repetitions of the Swing within 14 seconds. Well, then do you do another one or do you wait until the end of that 15 seconds? I encourage you not to obsess too much about those sorts of points. Rather you want to do as many repeats as you can in about a 15 second Epoch, and then rest for about 15 seconds, and then repeat for a total of 50 to 75 repetitions, Which might not sound like a lot to some of you might sound like an awful lot to others of you. To me, it sounds like a lot. you know. 50 repetitions of something and where you're trying to concentrate in your mind's eye on getting something accomplished over and over over again in exactly the same way, Might seem like a lot we know, based on the learning literature that your ability to successfully perform something in the real world will lend itself to better performance of that thing in the imagined world within your mind's eye. That's also one of these sort of does. But if you're trying to get better at something that you've never performed before, you really should know that the mental training visualization is probably not the best augment to that real world training until you're able to perform it successfully in the real world. At least some of the time, mental training and visualization can be effective, however, at increasing the accuracy or the frequency at which you can do that real world Behavior. So if normally you're only getting the correct swing or you're only hitting the the golf ball correctly, say 10 of the time. Mental training and visualization can really help bring that number up. But it is important that you are able to successfully complete that motor task in the real world. Similarly, for performance of cognitive tasks, so say, for instance, Um, speaking a new language, you might ask. Well, gosh, what. What? in the landscape of speaking a new language can be restricted to 5 to 15 seconds, where I could repeat it anywhere from you know, through one to three times in a given Epoch, and then rest and then keep repeating 50 to 75 times. Well there, I would encourage you to pick something that you are able to do, perhaps very slowly, so to speak a particular sentence, but with some challenge in getting the accent and the enunciation right, but you've completed it successfully before, and you want to get more smooth or more fluid with it. Likewise, for you know, playing piano or guitar again, you have to translate to the specific cognitive and or motor activity that you are seeking to improve at, But those epochs lasting 5 to 15 seconds are really the Cornerstone of an effective mental training and visualization practice, and the repeated nature of it, 50 to 75 repetitions in a given session is also another Cornerstone of an effective mental training and visualization practice. So says this review and some of the other papers that I'm going to get to in a few moments. Now, one of the other key components of a successful mental training and visualization practice is how often you perform that mental training and visualization practice. and again, a number of different Studies have looked at this through a number of different lenses, meaning anywhere from two to eight times per week. It does appear that performing these sessions anywhere from three to five times per week is going to be effective. We could perhaps even say most effective, because most of the uh, let's just call it the strongest data really point to repeating these 50 to 75 Trials of the same thing three to five times per week, so you can come up with a number that's reasonable for you to do consistently, and you might ask. Do you have to continue to perform the mental training and visualization forever? And the good news is the answer to that question is no. It does seem that once you have what's called Consolidated the Motor Performance or the cognitive performance of something, it can be further supported or reinforced that is Consolidated in the neural circuits that are responsible for performing that mental or physical task, So, in other words, once you are performing that cognitive or motor task in a way that's satisfactory, or perhaps just improved, Perhaps you're not a hundred percent, but it's improved in the real world. You don't need to continue to do mental training and visualization to to maintain that real world performance. So that's a good thing. In fact, the ideal situation would be then to pick a different sequence or thing that you're trying to learn and do mental training and visualization. For that, I perhaps might have misspoke there. Although I don't want to edit this out, I misspoke in the sense that again, I said, For the thing that you're trying to learn, remember, mental training and visualization is going to be most effective for building up the number of accurate trials or that your ability to do something with a greater frequency of something that you're already capable of doing, or have done at least once in the real world, Okay, this is not to say that mental training and visualization can't be used to acquire new skills. it can, in principle, but it has been shown to be most effective for enhancing the speed and the accuracy of skills that one has already demonstrated some degree of proficiency at. in the real world, I think that's important to point out because we often hear mental training visualization, and this equivalence of perceptual and motor experiences in our Mind's Eye to the real world, and we think, Oh, all we have to do is Imagine doing something, and we will get better at it, And unfortunately that's not the case. The good news is, however, if you can do something once even very slowly in the real world, and then you bring it to the mental imagery and visualization domain, you can get much faster at it in a way that really does translate back to the real world. I'd like to just take a brief moment and thank one of our podcast sponsors, which is Inside Tracker. Inside Tracker is a personalized nutrition platform that analyzes data from your blood and Dna to help you better understand your body and help you reach your health goals. I've long been a believer in getting regular blood work done for the simple reason that blood work is the only way that you can monitor the markers, such as hormone markers, lipids, metabolic factors, Etc, the impact your immediate and long-term Health. One major challenge with blood work, however, is that most of the time it does not come back with any information about what to do in order to move the values for hormones, metabolic factors, lipids, Etc into the ranges that you want with inside tracker. changing those values becomes very straightforward because it has a personalized dashboard that you can use to address the nutrition-based behavior-based supplement-based approaches that you can use in order to move those values into the ranges that are optimal for you. Your vitality and your longevity Inside Tracker now includes a measurement of Apo lipoprotein B. so-called Apo B. In their ultimate plan, Apob is a key marker of cardiovascular health, and therefore there's extreme value to knowing your Apob levels. If you'd like to try inside tracker, you can go to Insidetracker.com Huberman to get 20 off. Any of inside trackers Plans again. That's inside tracker.com Huberman to get 20 off Now, If you recall Principle number Three, or what I'm calling principle number Three of effective mental training and visualization, which was that you have to be able to perform the thing that you're trying to get better at through visualization and imagery in the real world. That should raise the question of what is the ratio of real world training versus mental training. That's going to be most effective. Well here, there's some really interesting data not just in the review that I mentioned, but in a couple of the other papers that we're going to talk about in a few minutes. But what I've done is I've synthesized the information across those papers, and they really all point to the fact that real world training is more effective than mental training, and mental training is more effective than no training. Now, the mental training more effective than no training is kind of a duh except that there are people, for instance, people who are injured who are trying to maintain or replenish some motor skill or ability to move in a particular way, or who have had traumatic brain injury and are trying to recreate experiences in a way that's safe for them while in a somewhat restricted format, So for instance, if you've damaged a limb or you're experiencing chronic pain and you need to take a layoff from some physical activity. There are now many studies looking at stroke patients at patients that have been in accidents. Tbi, also people who are suffering from more conventional limb and connective tissue injuries that if they do mental training, it obviously is not going to put them at risk of doing those same movements as it would in the real world, right, but that it can actually accelerate or at least maintain skill performance. So this is pretty exciting. If you think about it what this means and the reason it underscores this mental training is better than no training is that, should you find yourself in the unfortunate circumstance of being injured or unable to perform a given Behavior, Imagining the sequence of behavior that you'd like to maintain or even build up over time, Provided you've done that motor sequence before in the real world. Well, the mental training and visualization can really help keep that online or even help you improve over time. In fact, I have a colleague in the psychology department at Stanford who told me an anecdote, and admittedly it's just an anecdote of a student who was recruited to Stanford both for their academic prowess, but also for their abilities in tennis, and was injured in their first year and at first thought this was devastating, but did a cognitive reframe around the idea that that, let's call it extended layoff from actual tennis was going to afford them the ability to do more mental training than they would otherwise, even though they were quite sad to not be able to do actual physical training for tennis, and when they came back from that injury, they did indeed manage to improve beyond the initial non-injured state that we're in before the injury, which is pretty remarkable, but as Kelly pointed out to me, they were very careful to include a lot of mental training and visualization during that quote-unquote layoff period. So again, mental training better than no training. Physical training better than mental training. But when we say physical training better than mental training, what we're really talking about is when you allocate a certain amount of training hours for a given skill per week. Okay, so how would this look what these Studies have done? Is they've said Okay if people have the option of doing the real world training for 10 hours a week versus mental training for 10 hours a week. Which group performs better? It turns out it's the ones that do the physical training for 10 hours per week. However, we also know that combinations of physical training and mental training can bring about results that are greater than either one of those alone. How would that work Well? I wish I could tell you that if you did nine hours of physical training per week plus one hour of mental training that your performance would be better than if you did 10 hours of physical training And that's not the case. Okay. This is why we can reliably say physical Real World Training and again, this could be in the cognitive domain, is always going to be more effective on an hour by hour basis compared to mental training. So if you can do real world training, and perhaps we should be calling it real world, as opposed to physical. But if you can do real world training compared to purely mental training, that's going to be the best use of your time. This is really important. It doesn't underscore everything that we're talking about, because here's the really cool thing. if you do 10 hours per week of real world physical training again, Could be running. Could be music, could be, math, could be whatever it is. you're trying to learn shooting, basketball's hitting golf balls, and you add one hour or even half an hour of mental training to that real world training. Well then the results are significantly greater than you would experience with physical training alone, and of course that would be greater than you could achieve with mental training alone, because we already established that real world training is more powerful in learning skills and retaining skills than is mental training. Okay. if any of that was confusing, let me just say it one more time just to be ultra clear. If you have the option to do real world training for a cognitive, indoor, motor skill versus mental training, always go with real world training. However, if you can add to a maximum amount of real world training by doing some mental training and you follow the principles that we've been discussing here, which are gleaned from the scientific literature. Well then you are going to get significantly greater results in terms of speed, accuracy and consistency of performance of those real world behaviors and cognitive abilities. And, of course, if you are unable to do physical training for whatever reason, injury travel, whatever the case may be well, then doing mental training is still four, significantly greater than doing no training at all. Okay, so total layoffs, it turns out are a bad thing if you want to get better at something, and indeed, if you want to retain certain skills, both cognitive and motor. now, a couple of other things to keep in mind, as you're thinking about how to build up skills through a combination of physical and mental training. Well, remember back to the beginning of the episode where we talked about neuroplasticity and the fact that self-directed adaptive plasticity, which is really what we're talking about Here in this entire episode, Things that you're trying to learn in a deliberate way. That is, as you recall, a two-part process requires focused attention both when you're doing it in the real world and when you're doing mental training, and it requires rest and sleep, and in fact, you would be very wise to try and get a good night's sleep both on the days when you do physical training again, also called Real world training and mental training. you may also be asking, Can you do them on the same day? And this gets into some Nuance in the literature, But by my read of the literature, here's the takeaway. If you are doing the maximum amount of physical training that you can do according to your schedule, preventing injury and all those sorts of important constraints. and you're going to add mental training and imagery. It doesn't really matter when you do it. You could do it immediately after your physical training. You could do it on a separate day, but you do want to place it at a time in which you can try and get good sleep that night, so for instance, Believe It or Not, Studies have been done where people are doing mental training at times when they should be sleeping. That is going to offset some of the degradation and performance that you would normally see. but it's generally a bad idea. You should do your real world training and your mental training whenever it is that you can, and then you should try and get as much quality sleep as you possibly can on the night following that physical and or mental training. Okay, this is true of pretty much every night of your life, Right if I had my way. That is if I had a magic wand, which obviously I don't. I would ensure that I and everyone else in the world get sufficient amounts of quality sleep every single night, But that's just not realistic. There are going to be times where that's simply not going to happen for whatever reason, And I always say if you're not going to get sufficient amounts of quality sleep for whatever reason, trying to make it for a fund reason or a good reason, but I think getting sufficient amounts of quality sleep 80 percent of the nights of your life is a reasonable goal and one that's worth driving toward. And we have lots of episodes now or three really on mastering sleep, On perfecting your sleep, and uh episode guest episode with the great Matthew Walker, who wrote the book Why you Why we sleep? Incredibly important book. All of those as well as our toolkit for Sleep, describe ways to improve your sleep so you can refer to those episodes if you're having challenges with sleep and want to improve on sleep, and things like non-sleep depressed, which can support your ability of sleep and your ability to learn. so sleep is still vitally important not just for ensuring neuroplasticity occurs following real world training, but also following mental training and again, when you place, that mental training is not so critical. or at least it doesn't appear to be based on the literature. So if anyone out there has knowledge of any peer-reviewed studies stating that mental training should be done either before or after or some hours away from Real World Training, please send that to me or put in the. Excuse me. Put it in the comments on Youtube, and I'll see it there, because I do read all the comments, but I'm not aware of any any such data or analysis, and by the way, if you are interested in understanding the relationship between motor skill acquisition and retention and this first night phenomenon of sleep, the first night after training versus sleep on the second eye, Etc, there's a really wonderful paper that was published by none other than the great Matthew Walker, when I believe he was a graduate student. Maybe he was a postdoc when he did this in Robert's Stick Gold's Lab at Harvard. The title of the paper is sleep and Time Course of motor Skill learning. This is a paper published in 2003. Still an incredibly important paper. I will provide a link to it In the show note captions. It really highlights some of the key aspects of when people sleep, and how critical sleep is on the night following, and the nights following that training, in order to really consolidate certain types of learning and what phases of sleep relate to the consolidation of motor learning, Etc, a really wonderful paper. And of course, but just one of Matthew and Robert Stick Gold's incredible papers on sleep and learning. Remember at the beginning of the episode when I mentioned that many people are good at mental training and visualization, but some people are not well. Sex differences have been explored, and age-related differences have been explored in terms of people's ability to mentally visualize and train up specific skills. and while initially there were some sex differences identified really the bulk of the subsequent literature. that is the majority of quality. peer-reviewed studies on this aspect of mental training visualization point to the fact that there are no significant differences between males and females in terms of their ability to mentally visualize, nor their ability to use that mental visualization toward improving cognitive or motor skills. That point was covered in some detail in the review I mentioned earlier, Best practice for motor imagery, a systematic literature view on motor imagery, training elements in five different disciplines, This review also looked at age-related effects and perhaps the only thing that really popped out from this literature review in terms of age-dependent differences that point to changes in protocols that you might make, is that for individuals 65 or older, a combination of physical and mental training may actually allow them to gain and consolidate skills better than were they to do physical training alone. Now, Whether or not, that's due to some lower upper limit of physical training that they can do because of their age, or whether or not that's something specific to do with older versus younger neural circuits, isn't clear. But what this review also makes clear, is that for the vast majority of people out there, so teens, people in their 20s in their 40s, and so on, physical training more effective than mental training. We said that before combination of physical and mental training more effective than physical training alone, provided the mental training is on top of the maximum amount of physical training that one could do, and of course mental training more effective than no training at all. Okay, so we talked about sets and Reps. we talked about. You know, five to Fifteen second epochs with about 15 second breaks in between or rest between sets. If you will repeat it for 50 to 75 trials done three to five times per week. Some of the conditions of keeping it really simple. The Importance of Being able to actually perform those sequences in the real world, and so on, what we haven't discussed is first person versus third person and eyes open versus eyes closed. So what are we really talking about Here? Well, first person mental training and visualization would be where you are imagining doing something and you are seeing yourself doing something from the inside out, as opposed from the outside In Imagine, for instance, wearing a headcam Okay, or a body Cam, and doing something with your hands or being in virtual reality and having the sense that whatever you see in front of you and that's moving and that you're doing, that's you. So what I mean by this is a mental training or visualization protocol. For instance, if you were at the piano or at a guitar where you're actually looking down at or sensing the feeling of your hands, but you're not actually moving your hands Okay, as opposed to seeing yourself from outside of your body, So looking at yourself, say standing next to you or from across the room, You're looking at yourself playing the piano or playing guitar or swinging a golf club or doing a tennis. Serve okay, first person versus third person, And what the data tell us is that first person mental training and visualization is generally more effective than third person mental training and visualization, which perhaps raises another course of does out there, but it needn't have been the case right. I mean, you could imagine that seeing yourself doing something and doing it perfectly because you've done it perfectly once before, hopefully would allow you to build up that skill more quickly because you have that third person perspective where you can really see every aspect in every element of what you're trying to perform. Well, Turns out that the first person mental training and visualization is significantly more effective than that third person mental training and visualization, So if what you're trying to learn lends itself well to this first person mental experiencing of self as you perform the cognitive and or motor skill, I suggest you do that, as opposed to the third person version. Now what if what you're trying to learn doesn't lend itself well to first person visualization? For instance, what if you're trying to learn Uh, specific cognitive skill that doesn't involve any overt motor Behavior to be observed. Well, in that case, it's very clear that closing your eyes ideally and trying to perform that specific cognitive task or the statement, or the you know, uttering of a particular sentence in another language, or doing some sort of computation or problem solving of some sort in your head? Well, that itself, of course, is first person, because it's inside your own body, as opposed to, and I don't know that anyone would actually do this, But looking at yourself from a third person perspective in your mind's eye and seeing yourself perform that cognitive challenge. Whatever that challenge may happen to be okay. Now we have to address eyes open versus eyes closed. And this is where the literature gets pretty interesting. I always thought, for some reason, I don't know why, but I presumed that mental training and visualization should always be done eyes closed, But it turns out that's not how a lot of studies of mental training and visualization have been done. In fact, many of them have arrived at really impressive protocols, which are essentially the protocols that I've distilled out and I'm listing out during today's episode having people either watch videos of themselves performing a given skill and imagining themselves in that role and again. it's them. So again, during the mental training visualization, they're watching a movie of themselves, so they're somewhat in the third person perspective. I guess we could technically say they are in the third person perspective, but they're watching themself. So in doing that, we know based on neuroimaging studies that when we watch videos of ourselves doing things, we experience that more from a first-person perspective than if we watch videos of other people doing things. Use your imagination here, folks. So if you're somebody, for instance, who's trying to get better at a particular skill, this could be not just sport but also public speaking. Watching videos of yourself doing that can be very effective. but of course we have to come back to the first principle of effective mental training and visualization, which is that, whatever it is that we're trying to build up or consolidate as a skill needs to be brief and repeated. so what we're really talking about here is watching a video of ourselves on Loop, or listening to a audio or audio video recording of ourselves on Loop for whatever aspect that we're trying to build up or improve upon now for people, that, for instance, are trying to get better at dealing with public speaking, and there isn't a particular skill or utterance of particular sentences or words that they're trying to accomplish, but rather they're trying to learn to be more relaxed or to articulate better. in the public speaking scenario, There would be one of the few instances in which I suggest more General theme and not exact recapitulation of some specific words that you're going to say. Perhaps it could be a sequence of you walking out onto stage toward the podium, or out from the podium and facing the audience and looking in multiple directions up and down to see people in every corner of the room, and just repeating that on Loop in your mind's eye, or watching yourself do that on video and making yourself calm in your internal State as you're doing that, this is more of mental autonomic training, because what you're really trying to do is control your autonomic nervous system, the nervous system aspect that controls how alert or calm you are, as opposed to a specific skill. However, you could also translate this to you know, dance steps, or to motor sequences for playing an instrument, and so on. So the point here is that it's not as if there is zero utility to third person mental training and visualization There can be. but first person mental training visualization is going to be more effective as I mentioned before, and if you're going to use the third person mental training visualization, ideally, you would be looking at yourself either on video or listening to yourself in audio and or video. That is going to be more effective than closing your eyes and trying to imagine yourself from a third person perspective in your mind's eye. Okay, so just to make it really simple, first person better than third person visualization. If you're going to go with third person visualization, try and go with real third person visualization where you're actually seeing and or hearing yourself on a screen And again, This was somewhat of a surprise to me. I always thought that mental training and visualization was done with eyes closed. I thought Okay, Could you close your eyes? You imagine this, You imagine that. That's actually not the case for many many studies, some of which are considered real, Hallmark studies within the field of mental training and visualization, and the different neural circuits that it recruits, and along those lines, there's a really interesting study. It came out not that long ago. This was just a summer of 2022. Like to discuss in a little bit of detail because it really hammers home a number of the principles that we talked about. The title of the article is Mental practice modulates functional connectivity between the cerebellum and the primary motor cortex. I'm going to tell you the essential features of this study. First of all, primary motor cortex, sometimes called M1, is a relatively small but vitally important strip of neurons in or near the front of your brain. The neurons there are called upper motor neurons. They communicate through a set of neural connections with what are called lower motor neurons. The lower motor neurons sit in What's called the ventral Horn of the spinal cord, So along the spinal cord you have sensory inputs coming from skin and muscle, and what's called proprioceptive feedback that tells you where your limbs are in relation to each other and to yourself, and so on. You also have motor neurons that live in the spinal cord. They're actually the ones that send little wires that we call axons out to the muscles. release acetylcholine onto those muscles and allow those muscles to contract. Lower. motor neurons are the ones that actually generate movement. However, they are largely responsible for reflexive movements, are already learned movements and they require some input from things like Central pattern generators and some other circuits within the spinal cord and brain stem. But it's those M1 primary motor cortex neurons that are called upper motor neurons because they control lower motor neurons through directed action. Okay, so when I say primary motor cortex, I'm really talking about those upper motor neurons. M1, The cerebellum is an area in the back of your brain. If you were to look at a brain, you'd see two little lobes back there that are highly foliated. Foliated means that lots of lots of folds and lots of bumps and grooves back there, and actually means mini brain. It looks like a kind of a mini brain stuffed in the back of the brain. in certain animals, the cerebellum is much larger than the rest of the brain. In humans, the cerebellum is relatively small compared to the rest of the so-called neocortex, the outer shell, the human brain, The cerebellum is involved in Balance. It's also involved in eye movements. It's also involved in timing and motor learning. And the key thing to understand is that the cerebellum communicates with the primary motor cortex and it can do so through what's called inhibition. It has outputs that inhibit the activity of neurons in the motor cortex and elsewhere, and that has a profound influence on the execution of motor Behavior and the learning of particular motor behaviors. Now, I don't want to get into too much detail around all this, but what you need to know is that the cerebellum communicates with M1, primary motor cortex. M1 is primary motor cortex. So the upper motor neurons that are going to control that lower murder neurons and are going to control physical behavior and execution of physical movements. The communication between cerebellum and primary motor cortex is inhibitory, although it can activate motor cortex too, and this gets into a little bit of technical detail. but there can be inhibition of inhibition. So if you take something that's a break, and you inhibit that break, what you end up with is more excitation. Okay, So the takeaway here, that's key and everyone should be able to understand, even though you may or may not be following this whole Cerebellum Primary motor Cortex thing is that when we gain a new skill or we get more proficient at a skill so faster and more accurate, there tends to be more net excitation of the cerebellum to motor Cortex communication, and that is accomplished by reducing inhibition, so that's where it gets a little bit confusing to some. But in this paper what they did is they explored people's ability to improve on a very specific but very simple motor sequence. It's one that you're already familiar with. It's that tapping sequence that I talked about before where the thumb is digit one, index finger number two, middle finger number three, ring finger number four, and pinky finger number five, and it's a one, two, one, three, one, four, one, five, one, two, one, three, one, four, one five. And they had people actually perform this and they measured their speed and accuracy, and then they had them do a practice session that was either an intentional task. So one group just looked at an intentional cue and had to maintain Uh, focus on that attentional cue, and another group did mental practice. They basically did 50 imagined trials. so just in their Mind's Eye of this one, two, one, three, one, four, one, five on repeat. Okay 50 trials, much in the same way as what I referenced as the ideal protocol Earlier. Okay, 50 rounds of that. Then they got tested again on the motor task in the real world, and there were also recordings of the cerebellar to Primary motor Cortex communication. So there were a bunch of different results in this study, I think are interesting, but the ones that are most important are that quote. We found that mental practice enhanced both the speed and accuracy of this one, two, one, three, one, four, one five. performance in the real world, When people did these 50 imagine trials, There are many results out there, different papers that parallel and essentially say the same thing as what is said in this paper. And remember, there have been studies of mental training dating back to the 1880s. But what this paper really does, it looks at the neural machinery and the changes in the neural machinery, and what they found using transcranial magnetic stimulation, both in the context of stimulating, but also recording activity and connectivity between cerebellum and primary motor cortex is that mental training enhanced the net excitation of cerebellum to motor Cortex. communication. that is, it reduced the inhibition in a way that allowed motor cortex to generate these movements with more accuracy and more speed. What's also interesting about this paper is that it showed that the Improvement in performance of this task was not related to activation of the motor pathways themselves. So it's not the case that the cerebellum activation or inhibition changed the patterns of excitation going directly to the spinal cord, because those Pathways actually exist through a couple of intermediate stations. What it really showed is that when people do mental training and here you could say Okay, 50 trials. That's a lot of Trials, but it's not actually that many trials. It's pretty fast learning. If you think about do a task in the real world, Do 50 Trials of the Imagine task? Do the trial in the real world again? Significant Improvement in speed and accuracy through now, what are becoming to be established neural circuit connections between cerebellum and primary motor cortex? Okay, so this study is one of several, but not a tremendous number of studies out there that are starting to really pinpoint the underlying neural circuits that allow mental training and visualization to really improve motor skill performance. But again, and please hear me on this in this study and in the vast majority of other studies that have shown significant Improvement in Motor Performance in the real world by use of mental training visualization, there was an ability of each and everyone in the study to perform the specific motor sequence in the real world that then they were able to enhance with mental training and visualization. Now, thus far we've been talking mostly about performance of motor sequences and one of the things to really understand about performance of motor sequences both in the real world and in the Imagine context is that it involves the doing, that's what we call a go action, and not doing certain things What I mean by not doing well for many tasks out there, even ones as simple as the one, two, one, three, one, four, one five, task that we talked about a moment ago, there is the need not just to tap those fingers in the correct sequence as quickly as possible, but also to be accurate about it, to not do one, three, one four, or one, three and four at the same time, So there's both a Go component, an action component, and a withhold action component, and the ability to withhold action is strongly constrained by the time domain. In other words, the faster that we need to perform a given motor sequence, the more likely we are to perform incorrect components of the motor sequence as well, Okay, so one of the key things about mental training and visualization, that's really remarkable is that it can also be used and has been shown to improve not just go aspects of Motor Performance and cognitive performance, but also no go aspects of Motor Performance and skill learning. Now, the go no go thing is something I've discussed before on this podcast in reference to the so-called basal ganglia. Basal ganglia are subcortical, so they're below that bumpy surface of the human brain that we're most accustomed to seeing when we look at it from the outside, and the basal ganglia are strongly involved in Go versus No Go type tasks and learning. Now there are only a few studies that have really looked at the learning and the Improvement of No Go components of motor learning. But these no-go components are really really important. In fact, if we were to look at what's involved at Improvement in a golf swing or shooting free throws or getting better at piano or getting better at math or language speaking, I think it's fair to say that at least half and probably as much as 75 percent of motor learning is about restricting inappropriate movements or utterances or thoughts. If what you're trying to learn is purely cognitive, Okay, I think that's an important point that brings us back to our initial learning when we come into this world that developmental plasticity, which, as you recall, we have a lot of interconnected aspects of our brain and nervous system early in life, remember the example of the kid trying to eat and getting the spoon of food and Bowl on their head, Etc, and then over time getting more accurate at bringing food to their mouth and eating in a clean way, things that most but not all people accomplish in at some point in the course of their lifetime. Well, there haven't been many, but there have been a few very interesting studies looking at how mental training and visualization can improve the no-go aspect of motor learning. and I think this is important to highlight because it really mirrors What's Done in the real world, as opposed to just the finger tapping type things which are mostly go tasks again, there's a little bit of a no-go component there, but there are specific tasks that people have developed for the laboratory that really closely mimic action learning and cognitive learning in the real world, and one of the more important of those is What's called the stop signal task. Now the stop signal task is something that I'll explain to you. I'll also provide a link in the show note caption so you can try it. It's actually a lot of fun to try this because it really gives you a sense of just how challenging some of these laboratory tasks are. Let me just describe it. For a moment. The stop signal task was really developed and popularized by Gordon Logan and William Cowan. Gordon Logan is at Vanderbilt University and has done a lot of really important work. But one of the important aspects of his work is looking at Motor Performance and skill acquisition and the development of the stop signal task. I'll describe the stop signal task for you. Now, In Broad Contour, you or another research subject would sit in front of a screen. There are two keys on that keyboard or two keys among the other keys on that keyboard, one which is designated left, the other which is designated right, and then on the screen, you'll be presented for instance with a left-facing or a right-facing arrow. So in the initial trial, what would happen is that Arrow would pop up on the screen and your job is to press the left key. When the right facing arrow is presented, you press the right key. Okay, pretty straightforward, but there's a limited amount of time in which you can do this and the idea is that you're going to need to do this within approximately 500 milliseconds of the presentation of that Arrow, or else it's going to tell you that you missed that trial. Now. Of course, if you press the wrong key, so if the arrow goes left and you press the right key, then you would be told you got that one wrong. Okay So this is a reaction time test and not one that's particularly novel. What's novel? And what Logan and Cowan developed was that in the stop signal task, every once in a while, not every trial, but every once in a while, that arrow is presented, and then with some delay ranging from anywhere from 100 milliseconds to maybe 350 milliseconds, there will be a red circle or a red X also presented, which is a stop signal and your job is to not press the key that corresponds to the direction of arrow. In fact, not press any key at all. Now you can imagine how if the stop signal shows up with a longer delay after the presentation of the arrow, there's a higher probability that you will have already generated the key pressing movement. Okay, so at the link that we provide in the show note caption, you can actually do these two tasks and what you'll find is that you and most people will be able to do this Arrow to Reaction Time, pressing of the left to right key somewhere in the neighborhood between 300 milliseconds and maybe as long as 500 millisecond delay, you'll get an average of how quickly you respond, and then, of course, if you choose to, and I would hope you would choose to go on and do the stop signal task. you will be told trial by trial, whether or not you are hitting the right keys, Because if you are, you'll be allowed to progress to the next trial, or if you are told to stop, that is, you get the stop signal and you press the key anyway. you'll be told that you made an error because you did not stop now again with very short delays between the presentation of the arrow and the stop signal, you are going to be much better at inhibiting or preventing yourself from the behavior at the no go aspect of motor execution. That is what you will find is that if the stop signal is presented very shortly after, let's say 100 milliseconds, which is very, very brief amount of time after the presentation of the arrow, there's a good chance that you're going to be able to withhold the key pressing behavior. However, if the delay is anywhere from 200 to 350 milliseconds after the presentation of the arrow, chances are that you're going to press the button even when you shouldn't have on at least some of those trials. Okay, and if you try and game the system and wait a certain amount of time after the presentation of each Arrow, there will also be times in which the stop signal does not appear and you fail to hit the button in the appropriate amount of time. So it's a fun little task. It doesn't cost anything, or except maybe a couple of minutes of your time. and if you do have time to go to it, I think, Um, it will give you a much deeper flavor for the sorts of experiments that we're talking about here, and that you find that these stop signals are actually pretty hard to generate when you're trying to learn some new motor behavior, and that actually illustrates a bigger Point here. If today you sense that we've been talking about studies of you know, tapping fingers and you know stopping button presses, and that those examples are highly artificial and don't really translate to the real world. Well, keep in mind that the tasks that are used in these studies really Target the specific neural circuits. that is the same neural circuit that you would use for the performance of essentially any motor task. Now, of course, other motor tasks like ones where you involve your feet, or cognitive tasks where you have to think really hard about specific information and search for that information. Assemble it in particular ways. Of course, involve other neurons and neural circuits that we haven't discussed today. But the core components of these go and no-go tasks are the stop signal tasks Really capture the core elements of most all of cognitive and or motor learning. In some way That's fundamentally important. Okay, so they have real world relevance. The paper that I'd like to just briefly describe to you is entitled Motor Imagery combined with physical training improves response inhibition in the stop signal task. Okay, so that title is a little bit wordy, But now you know what the stop signal task is, and what this paper essentially found was that if people did physical training, so the sort of experiment that I just described versus mental training where they sat eyes open and imagined their responses to those arrows and stop signals, but they didn't actually generate any key presses versus a combination of the physical training, so the actual pressing of the buttons or withholding pressing of the buttons, as the case may be, plus mental training over the course of about five days, using the Contour described of the key principles of mental training performance we've talked about, I'll get to the specifics in a moment, but it really obeyed most all of what we've talked about, if not all of it. So repetition simple, repeated over about five days, and so on, and so forth, what they found was that the mental training and physical training group, so mental and real world training groups, performed significantly better in the stop signal reaction time. that is, they were able to withhold action when they needed to withhold action more frequently and with more accuracy than did either the physical training or mental training groups alone. So this actually fits in the face of what we said earlier, which is that physical training is always better than mental training, and mental training is always better than no training. And it's important to point out here that both the physical training and the mental training groups experience significant improvements in their reaction time and accuracy at the stop signal task. But in the case of this study which is exploring the withholding of inappropriate behaviors, the combination of mental training and physical training outperformed either physical or mental training alone. So while earlier, we said that if you have a certain amount of time in order to train something up, physical training is always going to be better than mental training. Well here we have somewhat of an exception where if the thing you're trying to learn involves withholding mistakes, as opposed to trying to generate the right behaviors per se. Well, then you are probably better off doing a combination of mental training and physical training. Let me state that a little bit differently. if you're finding that you're screwing up something, not because you can't initiate that particular motor Behavior, but you're doing the wrong thing at the wrong time. You're not able to withhold a particular action. Well then, in that case, mental training in combination with physical training becomes especially important. so for you, coaches for you students out there. keep that in mind when trying to learn how to withhold particular action sequences because they're not serving you well in the real world, Using a combination of real world training and physical training is actually better for you on an hour per hour basis than is physical training alone. A Couple of key details about this study. Should you decide to implement these protocols in this study, They did approximately 30 Trials of the thing that they were trying to get better at. Now they did those in the real world, So in this case the stop signal task involved actually pressing those buttons. And then they had a test phase of about 144 go trials and about 48 stop trials. Okay, so this is important If you are a coach or you're a student, or you're just going to self-direct this kind of learning in your self-directed adaptive plasticity. It's important that you mix in both go and no go trials. Okay, it wasn't always the case that there was a stop signal generated. The other thing that was really impressive about the study is that the changes occurred very quickly, so the training was performed five times over five days, so once a day for five days again. Back to this three to five times per week principle and the improvements were really significant. in some cases. In fact, if you decide to peruse this paper, you can go to Um. you know, Table two, you can see you know. in some cases a near doubling in the reduction in reaction time through a combination of mental and physical training compared to physical training alone or mental training alone again. However, both physical training and mental training groups alone saw significant improvements, but the combination of mental training and physical training was far greater than you saw with either one of those alone, so that's all nicely Quantified for you in this paper. So again, I really like this paper despite it not involving a huge number of subjects. I think it is a key paper because it really points to such an important element of motor learning and training, which is this action withholding component, this no-go component that here is captured so nicely in the stop signal task. So before we round up our discussion about motor training and visualization, I want to just briefly touch on some of the studies that have explored why certain individuals are better or worse at motor training and visualization, and what that might correlate with at the beginning of today's episode I briefly mentioned affantasia, which is this phenomenon where some people just simply can't or seem to have extreme Challenge generating visual imagery. There's been a number of studies exploring how aphantasics, as they're sometimes called, although nowadays it's um, not considered polite, if you will to refer to people according to their condition, So for instance, propasagnosia is a condition in which people are unable to recognize particular faces, And in the past these people were referred to as pro-pastagnosis Okay, as if their condition defined them, uh, right, um. Nowadays it's not considered polite to do that. Rather we say the person has professed agnosia or suffers from proposed signosia, Although the words suffer then also has become a little bit touchy. I'm going to do my best to just try and be as clear as possible here and explain that people who have aphantasia can have a Fantasia to varying degrees, so they can either have a complete absence of ability to generate mental imagery or they have a poor or kind of rudimentary ability to generate visual imagery in their Mind's Eye. It was thought that people who have aphantasia are not capable of what's called synesthesia synesthesias, or when people have perceptual blending, and this is not well under the influence of any kind of psychedelic or other kind of drug, perceptual blending of an atypical kind or rare kind, I actually have some friends. I have two friends that have different forms of synesthesia. One Associates different keys on the piano or musical notes with specific colors in a very very one-to-one specific way, so they'll tell you that E flat on the piano is a particular tone in their mind of of Amber, Hugh, Okay, and that I forget what other key is associated with a particular shade of red, and so on. And so forth. Are these people better at piano? Are they more perceptive of colors in their environment? Not necessarily so this is just a perceptual blending. It doesn't necessarily lend itself to any improved ability. Now you could imagine why people would hypothesize that people have aphantasia. Especially It's extreme form would not be capable of or have synesthesias. But it turns out that's not the case. There are a couple of really interesting papers again. We will link these in the show note. captions, Um, one is entitled What is the relationship between aphantasia synesthesia and autism, and the other one is aphantasia, the science of visual imagery extremes, And I really like the review at Fantasia, the science of visual imagery extremes, for those of you that are interested in understanding aphantasia with more depth. the study addressing the relationship between aphantasia synesthesia and autism, found that aphantasia is indeed linked to weak visual imagery, but that advantages can also be synesthetics, and vice versa. What was also interesting about this study is they addressed the question of whether or not people who have aphantasia, that is a challenge, or inability to generate mental or visual imagery, tend to have features associated with autism or residing somewhere on the autism spectrum. and I'm not trying to use ambiguous language here, but the whole set of language in nomenclature around autism and autism spectrum is also undergoing revision now because we are now coming to understand that autism, and nowadays it's generally not considered correct to call people autistics, in that sense, but autism is considered one set of positions along a spectrum that includes things like Asperger's, Etc, but that may also include other aspects of cognition and even personality. so these are starting to be viewed not just as a spectrum or one Continuum, ranging from you know, non-autistic to autistic, but a lot of variation in subtlety in between, and even crossing over with other aspects of Personality Psychology and Neuroscience. Okay, so I'm not trying to be vague here. I'm trying to be accurate, rather by saying, the whole description and categorization of autistic, non-autistic Etc, is undergoing vast revision right now, But the important point I think from this paper is that indeed, it was found that people who have a Fantasia tend to exhibit more of the features that are associated with the autism spectrum. Now how those things relate to one another in terms of their clinical relevance isn't clear. and of course it is entirely unclear as to what's the chicken and what's the egg there, So you could imagine no pun intended. For instance, that people that are on the autism spectrum might be less proficient at generating visual imagery because they are exceedingly proficient at other things. You could also imagine that people are placed onto the autism spectrum as it's sometimes referred, to, or are associated with particular features on the autism spectrum, because, in a causal way of the aphantasia, and of course, it's extremely important to highlight that not all people that consider themselves or that people consider Autistic or that are on the autism spectrum, or Asperger's, or any variation thereof, necessarily have a Fantasia. Just as it is that not all people that are on the autism spectrum completely lack or even lack what's called theory of mind, which is the ability to sort of empathize And subscribe feelings and motivations of others when viewing the actions and perceived feelings of others. Okay, So what I just described hopefully doesn't come across as just a bunch of word soup. What I'm trying to pinpoint is that there does seem to be a relationship between one's ability to generate visual imagery and certain constellations of cognitive and emotional perception and behavior and vice versa. Okay, in a future episode, I promise to cover synesthesia and autism and some of the related cognitive and motor aspects of autism, and things like Asperger's, I'm going to feature an expert guess, or actually several expert guests in this area because it is a rapidly evolving and somewhat controversial field. Meanwhile, I think it's important to at least consider how mental training and visualization might relate to certain aspects of cognition and our ability to visualize things, not just in terms of other people's behavior, which is one of the common ways that people probe for autism and Asperger's, versus non-autistic and non-aspergers and so on, the so-called theory of Mind task, in effect, asking whether or not children or adults can really get in the mind of others. That's a typical task developed by Simon Baron Cohen, but also whether or not children and adults are capable of generating mental imagery in a really Vivid way, or whether or not they have minor or even extreme Challenge in doing so, and perhaps the most direct way to explain why I included this aspect of the discussion of mental training and visualization, as it relates to different cognitive phenotypes or neurocognitive phenotypes, such as autism, Asperger's, Etc, is because, if you think about motor skill execution or cognitive skill execution, and the relationship between mental training and visualization and motor skills or cognitive skills, that's all pretty straightforward when you're talking about finger tapping and go no-go tasks and learning piano and things of that sort, but in many many ways our learning of social cognition, our learning of how to behave in certain circumstances, what's considered normal or atypical, neurotypical and neuroatypical if you will, A lot of that is not just generated from the inside out, but it also involves observation and visualization of what are considered appropriate and inappropriate. Definitely placed in quotes. By the way, folks, I'm not placing judgment. I'm just saying appropriate and inappropriate for a given context behavior. In other words, social learning and social cognition is every bit as much a learned behavior and pattern of cognitive and motor patterns as is tapping fingers or withholding key presses in a go no-go task. It's just that it transmits into a domain that involves smiling versus frowning versus asking a question, versus staying silent, versus sitting still, versus fidgeting what's appropriate and when what's inappropriate, and when all of that is what we call social cognition and has direct parallels to everything we've been talking about up until this point. So today we did a deep dive, which is often the case on this podcast into mental training and visualization. During the course of the episode, I tried to lay down one by one the key components of an effective mental training and visualization practice, everything ranging from making sure that the practice involves brief epochs, repeats of specific sequences of motor and or cognitive behavior. That those be relatively simple, so that you can imagine them even if you're somebody who's not good at doing mental training and visualization, And I should mention that if you do mental training and visualization repeatedly over time, you get better at mental training and visualization. There's a what's called metaplasticity here, so it's not just about engaging neuroplasticity of particular circuits, it's also about getting better at engaging plasticity. So plasticity of plasticity, I also describe the key Importance of Being able to actually execute specific movements and cognitive tasks in the real world, if you want the mental training and visualization to be especially effective, And we talked about the importance of naming things. We talked about the importance of creating not just one but many parallels between real world training and mental training and visualization, and really, on the whole, what we established was that cognitive and or motor learning really is something that you should do in the real world as much as possible, but if you can't, due to injury or whatever conditions, using mental training is a reasonable substitute, but not a complete substitute. And if you can't do real world training for whatever reason, injury, or otherwise, that mental training is going to be better than no training at all, and of course we established that at least four withholding action in order to get better at a skill, a combination of physical training and mental training is going to be best, but that, if you're trying to learn a new skill and you're having challenges with performing that skill because of an inability to do the skill in the first place or on a consistent basis, Well then, on an hour-by-hour basis, you're best off investing your time into the physical training only incorporating mental training and visualization, if you are able to do that on top of the maximum amount of real world training that you're capable of doing, And of course we talked about the actual neural circuits and a bit about how the actual neuroplasticity occurs early in the episode. I mentioned long-term depression well in describing the improvements in no-go tasks. Those stop signal tasks. A lot of what's observed during those tasks is an improvement or rather an increase in long-term depression of specific neural connections. So my hope is that in learning about those basic neural circuits and plasticity mechanisms, and in learning about the critical importance of focus and attention during learning, both real world and imagined, as well as the importance of sleep and deep rest for really consolidating learning and the different tools, the various steps or principles of effective mental training and visualization that you now have a fairly coherent, or maybe even a very coherent picture of how to develop the best mental training and visualization protocols For you, I realize that everyone has different goals. Everyone has different time constraints. If you are somebody that's interested in developing a mental training and visualization protocol, So if you're a coach or teacher, or simply a learner, or you're trying to self-direct your own adaptive plasticity, I want to emphasize that the key components that we discussed today are essential to include, but I wouldn't obsess about whether or not a given Epoch is 15 or 20 seconds or even 25 seconds. I wouldn't obsess over whether or not you got 30 repetitions in, and then your mind drifted, or whether or not you could do the full 50 to 75, or whether or not even in your mind's eye you made some errors. What's been shown over and over again in this literature is that performing mental training and visualization repeatedly and in a very restricted way that makes it easier to perform those trials over and over and over again, and with a high degree of accuracy almost always. really, we can fairly say, and essentially every study where it's been explored has led to improvements in real world performance of both cognitive and or physical tasks. So if you're trying to learn anything at all, I do encourage you to explore motor training and visualization, because basically all the studies out there, in fact, I couldn't find one exception where some degree of improvement wasn't observed when people use motor training and visualization on a consistent basis, Even just the three to five times per week. these simple repeats over and over, so I don't want to over complicate or make it sound like mental training and visualization has to be performed in a very precise way, or that it has to be done perfectly each and every time, quite, To the contrary, what is clear is that mental training and visualization is a very effective way to improve real world performance. If you're learning from and or enjoying this podcast, please subscribe to our Youtube channel. That's a terrific zero cost way to support us. In addition, please subscribe to the podcast on both Spotify and Apple and on both Spotify and Apple. You can leave us up to a five-star review. if you have questions for us or comments about the podcast, Or guess you'd like me to feature on the Huberman Lab podcast. Please put those in the comments section on Youtube. I do read all the comments. Please also check out the sponsors mentioned at the beginning and throughout today's episode. That's the best way to support the Huberman Lab podcast. Not so much on today's episode, but on many previous episodes of The Uberman Lab podcast. we discuss supplements. While supplements aren't necessary for everybody, many people derive tremendous benefit from them for things like improving sleep for hormone augmentation and for improving Focus. The Huberman Lab podcast is proud to have partnered with momentous supplements. If you'd like to learn more about the supplements discussed on the Huberman Lab podcast, please go to live Momentous Spelledous.com Slash Huberman again. that's Libomentis.com Huberman. If you're not already following Huberman Lab on social media, I am Huberman lab on Linkedin, Facebook, Twitter and Instagram, and at all those places I cover content, some of which overlaps with the content of the Huberman Lab podcast, but much of which is distinct from content on the Huberman Lab podcast. So again, it's Huberman Lab. All social media platforms. Also, I know many of you are interested in summaries of podcasts and what we call tool kits which describe ideal toolkits and protocols for sleep, or ideal toolkit and protocols for neuroplasticity for deliberate cold exposure, Etc, for that reason, we've established What's called the neural network newsletter. This is a completely zero cost newsletter that you can sign up for by going to Hubermanlab.com Go to the menu. Scroll down to newsletter, and you sign up by providing your email. We do not share your email with anybody, and there are also some sample Pdfs of existing Huberman Lab podcast protocols again, ranging from neuroplasticity to sleep and other topics that we've covered in brief, one to three page summaries. thank you once again for joining me for today's discussion, all about the science and effective implementation of mental training and visualization, and last, but certainly not least thank you for your interest in science [music]\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Convert the result list to a string\n",
        "result_str = ' '.join(result)\n",
        "\n",
        "# Define the filename and path where you want to save the TXT file\n",
        "txt_filename = \"/content/result.txt\"\n",
        "\n",
        "# Save the result text to the TXT file\n",
        "with open(txt_filename, 'w') as file:\n",
        "    file.write(result_str)\n",
        "\n",
        "# Download the TXT file\n",
        "files.download(txt_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "RTnpplJsvS3O",
        "outputId": "0bf2257d-0ae8-4481-d945-3f4a9d50aabd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_25d0bf06-72de-4323-a8ee-0e773fb4d35f\", \"result.txt\", 128095)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}